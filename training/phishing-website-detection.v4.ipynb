{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12693261,"sourceType":"datasetVersion","datasetId":8021783},{"sourceId":12742804,"sourceType":"datasetVersion","datasetId":8055177},{"sourceId":12751295,"sourceType":"datasetVersion","datasetId":8060741}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Phishing Website Detection using CountVectorizer","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2025-08-10T12:21:42.746434Z","iopub.execute_input":"2025-08-10T12:21:42.747100Z","iopub.status.idle":"2025-08-10T12:21:45.555119Z","shell.execute_reply.started":"2025-08-10T12:21:42.747068Z","shell.execute_reply":"2025-08-10T12:21:45.554139Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import psutil\nimport gc\n\n\ndef get_memory_usage():\n    \"\"\"Get current memory usage\"\"\"\n    process = psutil.Process()\n    memory_info = process.memory_info()\n    memory_mb = memory_info.rss / 1024 / 1024\n    return memory_mb\n\n\ndef print_memory_usage(label=\"\"):\n    \"\"\"Print current memory usage with optional label\"\"\"\n    memory_mb = get_memory_usage()\n    print(f\"Memory usage {label}: {memory_mb:.2f} MB\")\n\n\ndef cleanup_memory():\n    \"\"\"Force garbage collection to free memory\"\"\"\n    gc.collect()\n    print(\"Memory cleanup completed\")\n\n\n# Print initial memory usage\nprint_memory_usage(\"(initial)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T12:21:45.556836Z","iopub.execute_input":"2025-08-10T12:21:45.557450Z","iopub.status.idle":"2025-08-10T12:21:45.564816Z","shell.execute_reply.started":"2025-08-10T12:21:45.557424Z","shell.execute_reply":"2025-08-10T12:21:45.563846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport joblib\nimport numpy as np\nfrom scipy.sparse import save_npz, load_npz, vstack\nimport gc\n\n\ndataset_path = \"/kaggle/input/phishing-website-webcode-dataset/phishing_complete_dataset.csv\"\nARTIFACT_DIR = \"/kaggle/working/count/\"\n\n# Create artifacts directory if it doesn't exist\nos.makedirs(ARTIFACT_DIR, exist_ok=True)\n\n# First, let's check the total number of rows in the dataset\ntotal_rows = sum(1 for line in open(dataset_path)) - 1  # subtract 1 for header\nprint(f\"Total rows in dataset: {total_rows}\")\n\n# Read a small sample first to understand the data structure\nsample_df = pd.read_csv(dataset_path, nrows=5)\nprint(f\"Dataset columns: {sample_df.columns.tolist()}\")\nprint(f\"Sample data:\")\nprint(sample_df.head())","metadata":{"execution":{"iopub.status.busy":"2025-08-10T12:21:45.565822Z","iopub.execute_input":"2025-08-10T12:21:45.566080Z","iopub.status.idle":"2025-08-10T12:24:11.629101Z","shell.execute_reply.started":"2025-08-10T12:21:45.566052Z","shell.execute_reply":"2025-08-10T12:24:11.627980Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Batch processing configuration\nBATCH_SIZE = 2000\nCHUNK_SIZE = 2000\n\n\ndef process_dataset_in_batches():\n    \"\"\"Process dataset in batches to manage memory efficiently\"\"\"\n\n    # Initialize variables to track dataset statistics\n    total_processed = 0\n    class_counts = {}\n    code_lengths = []\n\n    print(\"Processing dataset in batches to analyze structure...\")\n\n    # Process dataset in chunks for analysis\n    chunk_iter = pd.read_csv(dataset_path, chunksize=CHUNK_SIZE)\n\n    for i, chunk in enumerate(chunk_iter):\n        print(\n            f\"Processing chunk {i+1}, rows {total_processed+1} to {total_processed+len(chunk)}\")\n\n        # Update class distribution\n        chunk_classes = chunk['result'].value_counts()\n        for class_label, count in chunk_classes.items():\n            class_counts[class_label] = class_counts.get(\n                class_label, 0) + count\n\n        # Sample some code lengths (to avoid memory issues)\n        if len(code_lengths) < 10000:  # Only sample first 10k for statistics\n            chunk_lengths = chunk['webpage_code'].str.len()\n            code_lengths.extend(chunk_lengths.tolist())\n\n        total_processed += len(chunk)\n\n        # Clear chunk from memory\n        del chunk\n        gc.collect()\n\n        if total_processed >= 10000:  # Limit analysis to first 10k rows for speed\n            break\n\n    print(f\"\\nDataset Analysis (first {total_processed} rows):\")\n    print(f\"Total processed: {total_processed}\")\n    print(f\"Class distribution: {class_counts}\")\n\n    if class_counts:\n        total_samples = sum(class_counts.values())\n        for class_label, count in class_counts.items():\n            percentage = (count / total_samples) * 100\n            print(f\"Class {class_label}: {count} ({percentage:.2f}%)\")\n\n    if code_lengths:\n        code_lengths_array = np.array(code_lengths)\n        print(f\"\\nWebpage code length statistics:\")\n        print(f\"Mean: {code_lengths_array.mean():.2f}\")\n        print(f\"Std: {code_lengths_array.std():.2f}\")\n        print(f\"Min: {code_lengths_array.min()}\")\n        print(f\"Max: {code_lengths_array.max()}\")\n        print(f\"Median: {np.median(code_lengths_array):.2f}\")\n\n\n# Run the analysis\nprocess_dataset_in_batches()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T12:24:11.630410Z","iopub.execute_input":"2025-08-10T12:24:11.631295Z","iopub.status.idle":"2025-08-10T12:24:28.458732Z","shell.execute_reply.started":"2025-08-10T12:24:11.631268Z","shell.execute_reply":"2025-08-10T12:24:28.457920Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_train_test_split_batched():\n    \"\"\"Create train-test split while processing dataset in batches\"\"\"\n\n    print(\"Creating train-test split with batched processing...\")\n\n    # First pass: collect all data for proper stratified split\n    all_X = []\n    all_y = []\n\n    chunk_iter = pd.read_csv(dataset_path, chunksize=CHUNK_SIZE)\n\n    for i, chunk in enumerate(chunk_iter):\n        print(f\"Reading chunk {i+1} for train-test split...\")\n        all_X.extend(chunk['webpage_code'].tolist())\n        all_y.extend(chunk['result'].tolist())\n        del chunk\n        gc.collect()\n\n    print(f\"Total samples loaded: {len(all_X)}\")\n\n    # Convert to pandas Series for train_test_split\n    X_series = pd.Series(all_X)\n    y_series = pd.Series(all_y)\n\n    # Create stratified train-test split\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_series, y_series, test_size=0.2, random_state=42, stratify=y_series\n    )\n\n    print(f\"Training set size: {len(X_train)}\")\n    print(f\"Test set size: {len(X_test)}\")\n    print(f\"Training target distribution:\\n{y_train.value_counts()}\")\n    print(f\"Test target distribution:\\n{y_test.value_counts()}\")\n\n    # Clear original data from memory\n    del all_X, all_y, X_series, y_series\n    gc.collect()\n\n    return X_train, X_test, y_train, y_test\n\n\n# Create the train-test split\nX_train, X_test, y_train, y_test = create_train_test_split_batched()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T12:24:28.461024Z","iopub.execute_input":"2025-08-10T12:24:28.461320Z","iopub.status.idle":"2025-08-10T12:27:04.112125Z","shell.execute_reply.started":"2025-08-10T12:24:28.461282Z","shell.execute_reply":"2025-08-10T12:27:04.111295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load previously processed CountVectorizer data if needed\n# Uncomment the lines below to load previously saved CountVectorizer data instead of processing\n# loaded_count_data = load_count_processed_data()\n# if loaded_count_data:\n#     count_vectorizer = loaded_count_data['vectorizer']\n#     X_train_count = loaded_count_data['X_train_count']\n#     X_test_count = loaded_count_data['X_test_count']\n#     y_train = loaded_count_data['y_train']\n#     y_test = loaded_count_data['y_test']\n#     print(\"‚úÖ Loaded previously processed CountVectorizer data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T12:27:04.113095Z","iopub.execute_input":"2025-08-10T12:27:04.113367Z","iopub.status.idle":"2025-08-10T12:27:04.117685Z","shell.execute_reply.started":"2025-08-10T12:27:04.113339Z","shell.execute_reply":"2025-08-10T12:27:04.116865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def vectorize_data_count_in_batches(X_train, X_test, y_train, y_test):\n    \"\"\"Vectorize training and test data using CountVectorizer in batches to manage memory\"\"\"\n\n    print(\"Starting CountVectorizer batch processing...\")\n    print(f\"‚úì Count artifacts will be saved to: {ARTIFACT_DIR}\")\n\n    # Initialize Count vectorizer\n    count_vectorizer = CountVectorizer(\n        max_features=5000,\n        stop_words='english',\n        ngram_range=(1, 2)\n    )\n\n    # Convert to lists for batch processing\n    X_train_list = X_train.tolist() if hasattr(\n        X_train, 'tolist') else list(X_train)\n    X_test_list = X_test.tolist() if hasattr(X_test, 'tolist') else list(X_test)\n\n    # Fit vectorizer on training data in batches\n    print(\"Fitting CountVectorizer on training data...\")\n\n    # For fitting, we need to process all training data\n    # We'll do this in batches but accumulate the vocabulary\n    train_batches = [X_train_list[i:i+BATCH_SIZE]\n                     for i in range(0, len(X_train_list), BATCH_SIZE)]\n\n    print(f\"Processing {len(train_batches)} training batches...\")\n\n    # Fit on the first few batches to establish vocabulary\n    # Use up to 10k samples for fitting\n    sample_size = min(10000, len(X_train_list))\n    sample_data = X_train_list[:sample_size]\n    count_vectorizer.fit(sample_data)\n\n    print(\"CountVectorizer fitted. Now transforming training data in batches...\")\n\n    # Transform training data in batches and save\n    X_train_count_batches = []\n    for i, batch in enumerate(train_batches):\n        print(f\"Transforming training batch {i+1}/{len(train_batches)}\")\n        batch_count = count_vectorizer.transform(batch)\n        X_train_count_batches.append(batch_count)\n\n        # Save batch to disk to free memory\n        batch_filename = os.path.join(\n            ARTIFACT_DIR, f\"X_train_count_batch_{i}.npz\")\n        save_npz(batch_filename, batch_count)\n\n        # Clear batch from memory\n        del batch_count\n        gc.collect()\n\n    print(\"Training data CountVectorization completed. Now processing test data...\")\n\n    # Transform test data in batches\n    test_batches = [X_test_list[i:i+BATCH_SIZE]\n                    for i in range(0, len(X_test_list), BATCH_SIZE)]\n    X_test_count_batches = []\n\n    for i, batch in enumerate(test_batches):\n        print(f\"Transforming test batch {i+1}/{len(test_batches)}\")\n        batch_count = count_vectorizer.transform(batch)\n        X_test_count_batches.append(batch_count)\n\n        # Save batch to disk\n        batch_filename = os.path.join(\n            ARTIFACT_DIR, f\"X_test_count_batch_{i}.npz\")\n        save_npz(batch_filename, batch_count)\n\n        # Clear batch from memory\n        del batch_count\n        gc.collect()\n\n    print(\"Combining all training batches...\")\n    # Load and combine all training batches\n    X_train_count = None\n    for i in range(len(train_batches)):\n        batch_filename = os.path.join(\n            ARTIFACT_DIR, f\"X_train_count_batch_{i}.npz\")\n        batch_data = load_npz(batch_filename)\n\n        if X_train_count is None:\n            X_train_count = batch_data\n        else:\n            X_train_count = vstack([X_train_count, batch_data])\n\n        # Clean up temporary file\n        os.remove(batch_filename)\n\n    print(\"Combining all test batches...\")\n    # Load and combine all test batches\n    X_test_count = None\n    for i in range(len(test_batches)):\n        batch_filename = os.path.join(\n            ARTIFACT_DIR, f\"X_test_count_batch_{i}.npz\")\n        batch_data = load_npz(batch_filename)\n\n        if X_test_count is None:\n            X_test_count = batch_data\n        else:\n            X_test_count = vstack([X_test_count, batch_data])\n\n        # Clean up temporary file\n        os.remove(batch_filename)\n\n    print(f\"CountVectorization completed!\")\n    print(f\"Training data shape: {X_train_count.shape}\")\n    print(f\"Test data shape: {X_test_count.shape}\")\n\n    # Clear original text data from memory\n    del X_train_list, X_test_list\n    gc.collect()\n\n    return count_vectorizer, X_train_count, X_test_count","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T12:27:04.118670Z","iopub.execute_input":"2025-08-10T12:27:04.118913Z","iopub.status.idle":"2025-08-10T12:27:04.150472Z","shell.execute_reply.started":"2025-08-10T12:27:04.118890Z","shell.execute_reply":"2025-08-10T12:27:04.149387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Process data with CountVectorizer in batches\nprint(\"Starting CountVectorizer batch processing...\")\nprint_memory_usage(\"(before vectorization)\")\n\n    # Call the CountVectorizer batch processing function\n    count_vectorizer, X_train_count, X_test_count = vectorize_data_count_in_batches(\n        X_train, X_test, y_train, y_test)\n    \n    # Clear original text data\n    del X_train, X_test\n    gc.collect()\n    \n    print_memory_usage(\"(after vectorization and cleanup)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T12:27:04.151557Z","iopub.execute_input":"2025-08-10T12:27:04.151882Z","iopub.status.idle":"2025-08-10T13:01:48.620178Z","shell.execute_reply.started":"2025-08-10T12:27:04.151844Z","shell.execute_reply":"2025-08-10T13:01:48.619363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_count_processed_data(count_vectorizer, X_train_count, X_test_count, y_train, y_test):\n    \"\"\"Save all CountVectorizer processed data and vectorizer to disk\"\"\"\n\n    print(\"Saving CountVectorizer processed data to disk...\")\n\n    # Save vectorizer\n    vectorizer_path = os.path.join(ARTIFACT_DIR, \"count_vectorizer.joblib\")\n    joblib.dump(count_vectorizer, vectorizer_path)\n    print(f\"‚úì Saved Count vectorizer to {vectorizer_path}\")\n\n    # Save vectorized training data\n    train_count_path = os.path.join(ARTIFACT_DIR, \"X_train_count.npz\")\n    save_npz(train_count_path, X_train_count)\n    print(f\"‚úì Saved training Count data to {train_count_path}\")\n\n    # Save vectorized test data\n    test_count_path = os.path.join(ARTIFACT_DIR, \"X_test_count.npz\")\n    save_npz(test_count_path, X_test_count)\n    print(f\"‚úì Saved test Count data to {test_count_path}\")\n\n    # Save target variables\n    y_train_np = y_train.to_numpy() if hasattr(\n        y_train, \"to_numpy\") else np.asarray(y_train)\n    y_test_np = y_test.to_numpy() if hasattr(\n        y_test, \"to_numpy\") else np.asarray(y_test)\n\n    y_train_count_path = os.path.join(ARTIFACT_DIR, \"y_train_count.npy\")\n    y_test_count_path = os.path.join(ARTIFACT_DIR, \"y_test_count.npy\")\n\n    np.save(y_train_count_path, y_train_np)\n    np.save(y_test_count_path, y_test_np)\n    print(f\"‚úì Saved training targets to {y_train_count_path}\")\n    print(f\"‚úì Saved test targets to {y_test_count_path}\")\n\n    # Print file sizes for verification\n    print(\"\\nCountVectorizer file sizes:\")\n    for filename in [\"count_vectorizer.joblib\", \"X_train_count.npz\", \"X_test_count.npz\", \"y_train_count.npy\", \"y_test_count.npy\"]:\n        filepath = os.path.join(ARTIFACT_DIR, filename)\n        if os.path.exists(filepath):\n            size_mb = os.path.getsize(filepath) / (1024 * 1024)\n            print(f\"  {filename}: {size_mb:.2f} MB\")\n\n    print(\n        f\"\\n‚úÖ All CountVectorizer data saved successfully to '{ARTIFACT_DIR}'\")\n\n    return {\n        'vectorizer_path': vectorizer_path,\n        'train_count_path': train_count_path,\n        'test_count_path': test_count_path,\n        'y_train_path': y_train_count_path,\n        'y_test_path': y_test_count_path\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T13:01:48.622113Z","iopub.execute_input":"2025-08-10T13:01:48.622363Z","iopub.status.idle":"2025-08-10T13:01:48.634782Z","shell.execute_reply.started":"2025-08-10T13:01:48.622343Z","shell.execute_reply":"2025-08-10T13:01:48.633738Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save all processed CountVectorizer data\nsaved_paths = save_count_processed_data(\n    count_vectorizer, X_train_count, X_test_count, y_train, y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T13:01:48.635679Z","iopub.execute_input":"2025-08-10T13:01:48.635937Z","iopub.status.idle":"2025-08-10T13:03:40.316267Z","shell.execute_reply.started":"2025-08-10T13:01:48.635912Z","shell.execute_reply":"2025-08-10T13:03:40.315163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_count_processed_data():\n    \"\"\"Load previously saved CountVectorizer processed data for model training\"\"\"\n\n    print(\"Loading CountVectorizer processed data from disk...\")\n\n    # Check if all required files exist\n    required_files = [\n        \"count_vectorizer.joblib\",\n        \"X_train_count.npz\",\n        \"X_test_count.npz\",\n        \"y_train_count.npy\",\n        \"y_test_count.npy\"\n    ]\n\n    missing_files = []\n    for filename in required_files:\n        filepath = os.path.join(ARTIFACT_DIR, filename)\n        if not os.path.exists(filepath):\n            missing_files.append(filename)\n\n    if missing_files:\n        print(f\"‚ùå Missing files: {missing_files}\")\n        print(\"Please run the CountVectorizer data processing cells first.\")\n        return None\n\n    # Load vectorizer\n    vectorizer_path = os.path.join(ARTIFACT_DIR, \"count_vectorizer.joblib\")\n    loaded_vectorizer = joblib.load(vectorizer_path)\n    print(f\"‚úì Loaded Count vectorizer from {vectorizer_path}\")\n\n    # Load vectorized data\n    train_count_path = os.path.join(ARTIFACT_DIR, \"X_train_count.npz\")\n    test_count_path = os.path.join(ARTIFACT_DIR, \"X_test_count.npz\")\n\n    loaded_X_train_count = load_npz(train_count_path)\n    loaded_X_test_count = load_npz(test_count_path)\n    print(f\"‚úì Loaded training Count data: {loaded_X_train_count.shape}\")\n    print(f\"‚úì Loaded test Count data: {loaded_X_test_count.shape}\")\n\n    # Load target variables\n    y_train_path = os.path.join(ARTIFACT_DIR, \"y_train_count.npy\")\n    y_test_path = os.path.join(ARTIFACT_DIR, \"y_test_count.npy\")\n\n    loaded_y_train = np.load(y_train_path)\n    loaded_y_test = np.load(y_test_path)\n    print(f\"‚úì Loaded training targets: {loaded_y_train.shape}\")\n    print(f\"‚úì Loaded test targets: {loaded_y_test.shape}\")\n\n    print(f\"\\n‚úÖ All CountVectorizer data loaded successfully!\")\n\n    return {\n        'vectorizer': loaded_vectorizer,\n        'X_train_count': loaded_X_train_count,\n        'X_test_count': loaded_X_test_count,\n        'y_train': loaded_y_train,\n        'y_test': loaded_y_test\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T13:03:40.317353Z","iopub.execute_input":"2025-08-10T13:03:40.317689Z","iopub.status.idle":"2025-08-10T13:03:40.327982Z","shell.execute_reply.started":"2025-08-10T13:03:40.317647Z","shell.execute_reply":"2025-08-10T13:03:40.326894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_models_on_count_data(X_train_count, X_test_count, y_train, y_test):\n    \"\"\"Train models using the CountVectorizer processed data\"\"\"\n\n    count_results = {}\n\n    # Define classifiers for Count data\n    classifiers_count = {\n        'Random Forest (Count)': RandomForestClassifier(n_estimators=100, random_state=42),\n        'XGBoost (Count)': xgb.XGBClassifier(random_state=42, verbosity=0),\n        'LightGBM (Count)': lgb.LGBMClassifier(random_state=42, verbose=-1),\n        'Extra Trees (Count)': ExtraTreesClassifier(n_estimators=100, random_state=42),\n        'Naive Bayes (Count)': MultinomialNB(),\n        'Logistic Regression (Count)': LogisticRegression(random_state=42, max_iter=1000)\n    }\n\n    print(\"Training and evaluating models on CountVectorizer data...\")\n    print(\"=\" * 60)\n    print(f\"Training data shape: {X_train_count.shape}\")\n    print(f\"Test data shape: {X_test_count.shape}\")\n    print(f\"Models will be saved to: {ARTIFACT_DIR}\")\n    print(\"=\" * 60)\n\n    # Train Count models\n    for name, clf in classifiers_count.items():\n        print(f\"\\nüöÄ Training {name}...\")\n        try:\n            # Train the model\n            clf.fit(X_train_count, y_train)\n\n            # Make predictions\n            y_pred = clf.predict(X_test_count)\n\n            # Calculate metrics\n            accuracy = accuracy_score(y_test, y_pred)\n            f1 = f1_score(y_test, y_pred)\n\n            # Store results\n            count_results[name] = {\n                'accuracy': accuracy,\n                'f1_score': f1,\n                'predictions': y_pred,\n                'model': clf\n            }\n\n            print(f\"   ‚úÖ Accuracy: {accuracy:.4f}\")\n            print(f\"   ‚úÖ F1 Score: {f1:.4f}\")\n\n            # Save the trained model\n            model_filename = f\"{name.replace(' ', '_').replace('(', '').replace(')', '').lower()}.joblib\"\n            model_path = os.path.join(ARTIFACT_DIR, model_filename)\n            joblib.dump(clf, model_path)\n            print(f\"   üíæ Saved model to {model_filename}\")\n\n        except Exception as e:\n            print(f\"   ‚ùå Error training {name}: {e}\")\n\n    print(f\"\\n{'='*60}\")\n    print(\"üéâ CountVectorizer model training completed!\")\n\n    return count_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T13:03:40.329005Z","iopub.execute_input":"2025-08-10T13:03:40.329323Z","iopub.status.idle":"2025-08-10T13:03:40.354802Z","shell.execute_reply.started":"2025-08-10T13:03:40.329300Z","shell.execute_reply":"2025-08-10T13:03:40.353842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_count_vectorizer_results(count_results):\n    \"\"\"Analyze and summarize CountVectorizer model performance\"\"\"\n\n    print(\"üîç COUNTVECTORIZER MODEL ANALYSIS\")\n    print(\"=\" * 50)\n\n    # Create analysis dataframe\n    analysis_data = []\n    for name, result in count_results.items():\n        model_name = name.replace(\" (Count)\", \"\")\n\n        analysis_data.append({\n            'Model': model_name,\n            'Accuracy': result['accuracy'],\n            'F1 Score': result['f1_score'],\n            'Full Name': name\n        })\n\n    analysis_df = pd.DataFrame(analysis_data)\n    analysis_df = analysis_df.sort_values('Accuracy', ascending=False)\n\n    print(\"üìä DETAILED RESULTS:\")\n    print(analysis_df[['Model', 'Accuracy', 'F1 Score']\n                      ].to_string(index=False))\n\n    # Find best model\n    best_model_row = analysis_df.iloc[0]\n    best_model_name = best_model_row['Full Name']\n    best_model = count_results[best_model_name]['model']\n\n    print(f\"\\nüèÜ BEST MODEL:\")\n    print(f\"   Model: {best_model_row['Model']}\")\n    print(f\"   Accuracy: {best_model_row['Accuracy']:.4f}\")\n    print(f\"   F1 Score: {best_model_row['F1 Score']:.4f}\")\n\n    # Performance statistics\n    print(f\"\\nüìà PERFORMANCE STATISTICS:\")\n    print(f\"   Average Accuracy: {analysis_df['Accuracy'].mean():.4f}\")\n    print(f\"   Best Accuracy: {analysis_df['Accuracy'].max():.4f}\")\n    print(f\"   Worst Accuracy: {analysis_df['Accuracy'].min():.4f}\")\n    print(f\"   Accuracy Std: {analysis_df['Accuracy'].std():.4f}\")\n    print(f\"   Average F1 Score: {analysis_df['F1 Score'].mean():.4f}\")\n    print(f\"   Best F1 Score: {analysis_df['F1 Score'].max():.4f}\")\n\n    # Visualize performance\n    plt.figure(figsize=(12, 8))\n\n    # Plot 1: Accuracy ranking\n    plt.subplot(2, 2, 1)\n    colors = plt.cm.Reds(np.linspace(0.4, 0.8, len(analysis_df)))\n    plt.barh(range(len(analysis_df)), analysis_df['Accuracy'], color=colors)\n    plt.xlabel('Accuracy')\n    plt.ylabel('Models')\n    plt.title('Model Ranking by Accuracy')\n    plt.yticks(range(len(analysis_df)), analysis_df['Model'])\n    plt.gca().invert_yaxis()\n\n    # Plot 2: F1 Score ranking\n    plt.subplot(2, 2, 2)\n    colors = plt.cm.Greens(np.linspace(0.4, 0.8, len(analysis_df)))\n    plt.barh(range(len(analysis_df)), analysis_df['F1 Score'], color=colors)\n    plt.xlabel('F1 Score')\n    plt.ylabel('Models')\n    plt.title('Model Ranking by F1 Score')\n    plt.yticks(range(len(analysis_df)), analysis_df['Model'])\n    plt.gca().invert_yaxis()\n\n    # Plot 3: Accuracy vs F1 scatter\n    plt.subplot(2, 2, 3)\n    plt.scatter(analysis_df['Accuracy'], analysis_df['F1 Score'],\n                c=range(len(analysis_df)), cmap='viridis', s=100)\n    plt.xlabel('Accuracy')\n    plt.ylabel('F1 Score')\n    plt.title('Accuracy vs F1 Score')\n    for i, model in enumerate(analysis_df['Model']):\n        plt.annotate(model, (analysis_df.iloc[i]['Accuracy'], analysis_df.iloc[i]['F1 Score']),\n                     xytext=(5, 5), textcoords='offset points', fontsize=8)\n\n    # Plot 4: Performance distribution\n    plt.subplot(2, 2, 4)\n    plt.hist([analysis_df['Accuracy'], analysis_df['F1 Score']],\n             bins=10, alpha=0.7, label=['Accuracy', 'F1 Score'])\n    plt.xlabel('Score')\n    plt.ylabel('Frequency')\n    plt.title('Performance Distribution')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    return analysis_df, best_model_name, best_model\n\n# Note: Use this function after training CountVectorizer models\n# analysis_df, best_model_name, best_model = analyze_count_vectorizer_results(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T13:03:40.356006Z","iopub.execute_input":"2025-08-10T13:03:40.356367Z","iopub.status.idle":"2025-08-10T13:03:40.380829Z","shell.execute_reply.started":"2025-08-10T13:03:40.356338Z","shell.execute_reply":"2025-08-10T13:03:40.379722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# üöÄ COMPLETE COUNTVECTORIZER WORKFLOW\n\ndef run_complete_count_vectorization_workflow():\n    \"\"\"\n    Complete workflow to process data with CountVectorizer and train models\n    Run this after the initial data loading and train-test split\n    \"\"\"\n\n    print(\"üéØ STARTING COUNTVECTORIZER WORKFLOW\")\n    print(\"=\" * 50)\n\n    # Check if we have the basic data\n    if 'X_train' not in globals() or 'X_test' not in globals():\n        print(\"‚ùå Train-test split data not found. Please run the data loading cells first.\")\n        return False\n\n    # Step 1: Process with CountVectorizer\n    print(\"\\nüìä STEP 1: CountVectorizer Processing\")\n    if 'count_vectorizer' not in globals():\n        print(\"Processing with CountVectorizer...\")\n        global count_vectorizer, X_train_count, X_test_count\n        count_vectorizer, X_train_count, X_test_count = vectorize_data_count_in_batches(\n            X_train, X_test, y_train, y_test)\n        save_count_processed_data(\n            count_vectorizer, X_train_count, X_test_count, y_train, y_test)\n    else:\n        print(\"‚úÖ CountVectorizer data already available\")\n\n    # Step 2: Train models on CountVectorizer data\n    print(\"\\nü§ñ STEP 2: Training CountVectorizer Models\")\n    if 'results' not in globals():\n        global results\n        results = train_models_on_count_data(\n            X_train_count, X_test_count, y_train, y_test)\n    else:\n        print(\"‚úÖ CountVectorizer models already trained\")\n\n    # Step 3: Analyze results\n    print(\"\\nüìà STEP 3: Analyzing Results\")\n    analysis_df, best_model_name, best_model = analyze_count_vectorizer_results(\n        results)\n\n    # Step 4: Test predictions\n    print(\"\\nüß™ STEP 4: Testing Predictions\")\n    test_prediction_functions()\n\n    print(\"\\nüéâ COUNTVECTORIZER WORKFLOW COMPLETED!\")\n    print(\"=\" * 50)\n\n    return True\n\n\n# Instructions for usage:\nprint(\"\"\"\nüìã COUNTVECTORIZER WORKFLOW INSTRUCTIONS:\n\n1. FIRST RUN (Complete Processing):\n   - Run cells 1-6 to load data and create train-test split\n   - Run: run_complete_count_vectorization_workflow()\n   - This will process data with CountVectorizer and train all models\n\n2. SUBSEQUENT RUNS (Load Saved Data):\n   - Uncomment and run the data loading section\n   - Load Count data: load_count_processed_data()\n   - Analyze results: analyze_count_vectorizer_results(results)\n\n3. INDIVIDUAL STEPS:\n   - Data processing: vectorize_data_count_in_batches()\n   - Model training: train_models_on_count_data()\n   - Results analysis: analyze_count_vectorizer_results()\n   - Prediction testing: test_prediction_functions()\n\nüí° MEMORY MANAGEMENT TIPS:\n- CountVectorizer processes data in 2000-row batches\n- Intermediate files are cleaned up automatically\n- Use gc.collect() between steps if memory is limited\n- All processed data is saved to {ARTIFACT_DIR} for future use\n\nüéØ COUNTVECTORIZER ADVANTAGES:\n- Simple and fast text vectorization\n- Preserves exact word frequencies\n- Less memory intensive during processing\n- Good baseline approach for text classification\n- Efficient for large datasets with batch processing\n\"\"\")\n\n# Uncomment the line below to run the complete workflow:\n# run_complete_count_vectorization_workflow()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T13:03:40.385020Z","iopub.execute_input":"2025-08-10T13:03:40.385423Z","iopub.status.idle":"2025-08-10T13:03:40.405170Z","shell.execute_reply.started":"2025-08-10T13:03:40.385386Z","shell.execute_reply":"2025-08-10T13:03:40.404117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# üìÅ COUNTVECTORIZER DATA ORGANIZATION\n\ndef show_count_directory_structure():\n    \"\"\"Display the CountVectorizer data directory structure\"\"\"\n    \n    print(\"üìÅ COUNTVECTORIZER DATA ORGANIZATION\")\n    print(\"=\" * 40)\n    \n    # Check CountVectorizer directory\n    count_dir = ARTIFACT_DIR\n    \n    print(f\"üî∏ CountVectorizer Directory: {count_dir}\")\n    if os.path.exists(count_dir):\n        count_files = [f for f in os.listdir(count_dir) if os.path.isfile(os.path.join(count_dir, f))]\n        print(f\"   ‚úì Contains {len(count_files)} files\")\n        if count_files:\n            total_size = 0\n            for file in sorted(count_files):\n                size_mb = os.path.getsize(os.path.join(count_dir, file)) / (1024 * 1024)\n                total_size += size_mb\n                print(f\"     ‚Ä¢ {file} ({size_mb:.2f} MB)\")\n            print(f\"   üìä Total size: {total_size:.2f} MB\")\n    else:\n        print(\"   ‚ùå Directory does not exist\")\n    \n    print(f\"\\nüìä EXPECTED DIRECTORY STRUCTURE:\")\n    print(f\"\"\"\n{ARTIFACT_DIR}\n‚îú‚îÄ‚îÄ count_vectorizer.joblib         # CountVectorizer\n‚îú‚îÄ‚îÄ X_train_count.npz               # Training features\n‚îú‚îÄ‚îÄ X_test_count.npz                # Test features\n‚îú‚îÄ‚îÄ y_train_count.npy               # Training labels\n‚îú‚îÄ‚îÄ y_test_count.npy                # Test labels\n‚îú‚îÄ‚îÄ random_forest_count.joblib      # Trained Random Forest model\n‚îú‚îÄ‚îÄ xgboost_count.joblib            # Trained XGBoost model\n‚îú‚îÄ‚îÄ lightgbm_count.joblib           # Trained LightGBM model\n‚îú‚îÄ‚îÄ extra_trees_count.joblib        # Trained Extra Trees model\n‚îú‚îÄ‚îÄ naive_bayes_count.joblib        # Trained Naive Bayes model\n‚îî‚îÄ‚îÄ logistic_regression_count.joblib # Trained Logistic Regression model\n\"\"\")\n\ndef verify_count_data_integrity():\n    \"\"\"Verify the integrity of CountVectorizer processed data\"\"\"\n    \n    print(\"\udd0d VERIFYING COUNTVECTORIZER DATA INTEGRITY\")\n    print(\"=\" * 45)\n    \n    required_files = [\n        \"count_vectorizer.joblib\",\n        \"X_train_count.npz\", \n        \"X_test_count.npz\",\n        \"y_train_count.npy\",\n        \"y_test_count.npy\"\n    ]\n    \n    missing_files = []\n    existing_files = []\n    \n    for filename in required_files:\n        filepath = os.path.join(ARTIFACT_DIR, filename)\n        if os.path.exists(filepath):\n            existing_files.append(filename)\n            # Check file size\n            size_mb = os.path.getsize(filepath) / (1024 * 1024)\n            print(f\"‚úì {filename} ({size_mb:.2f} MB)\")\n        else:\n            missing_files.append(filename)\n            print(f\"‚ùå {filename} - MISSING\")\n    \n    print(f\"\\nüìä SUMMARY:\")\n    print(f\"   Found: {len(existing_files)}/{len(required_files)} required files\")\n    \n    if missing_files:\n        print(f\"   ‚ùå Missing files: {missing_files}\")\n        print(\"   Please run the CountVectorizer processing pipeline first.\")\n        return False\n    else:\n        print(\"   ‚úÖ All required files present\")\n        \n        # Check for trained models\n        model_patterns = [\n            \"random_forest_count.joblib\",\n            \"xgboost_count.joblib\", \n            \"lightgbm_count.joblib\",\n            \"extra_trees_count.joblib\",\n            \"naive_bayes_count.joblib\",\n            \"logistic_regression_count.joblib\"\n        ]\n        \n        model_count = 0\n        for pattern in model_patterns:\n            if os.path.exists(os.path.join(ARTIFACT_DIR, pattern)):\n                model_count += 1\n        \n        print(f\"   ü§ñ Trained models: {model_count}/{len(model_patterns)}\")\n        \n        return True\n\ndef cleanup_count_directory():\n    \"\"\"Clean up temporary or unnecessary files in the CountVectorizer directory\"\"\"\n    \n    print(\"üßπ CLEANING UP COUNTVECTORIZER DIRECTORY\")\n    print(\"=\" * 40)\n    \n    if not os.path.exists(ARTIFACT_DIR):\n        print(\"‚ùå CountVectorizer directory does not exist\")\n        return\n    \n    # Patterns for temporary files to clean up\n    temp_patterns = [\n        \"*_batch_*.npz\",  # Temporary batch files\n        \"*.tmp\",          # Temporary files\n        \"*.log\"           # Log files\n    ]\n    \n    import glob\n    cleaned_count = 0\n    \n    for pattern in temp_patterns:\n        temp_files = glob.glob(os.path.join(ARTIFACT_DIR, pattern))\n        for temp_file in temp_files:\n            try:\n                os.remove(temp_file)\n                print(f\"üóëÔ∏è Removed: {os.path.basename(temp_file)}\")\n                cleaned_count += 1\n            except Exception as e:\n                print(f\"‚ùå Failed to remove {os.path.basename(temp_file)}: {e}\")\n    \n    if cleaned_count == 0:\n        print(\"‚úÖ No temporary files to clean up\")\n    else:\n        print(f\"‚úÖ Cleaned up {cleaned_count} temporary files\")\n\n# Display current structure\nshow_count_directory_structure()\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"üí° COUNTVECTORIZER ORGANIZATION TIPS:\")\nprint(\"1. Run verify_count_data_integrity() to check data completeness\")\nprint(\"2. Run cleanup_count_directory() to remove temporary files\") \nprint(\"3. All CountVectorizer data is stored in a single directory\")\nprint(\"4. Models are saved with descriptive names for easy identification\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train all models using CountVectorizer data\nresults = train_models_on_count_data(\n    X_train_count, X_test_count, y_train, y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compare CountVectorizer model performances\nprint(\"CountVectorizer Model Performance Comparison:\")\nprint(\"=\" * 60)\n\n# Create comparison dataframe\ncomparison_data = []\nfor name, result in results.items():\n    comparison_data.append({\n        # Remove (Count) suffix for cleaner display\n        'Model': name.replace(' (Count)', ''),\n        'Accuracy': result['accuracy'],\n        'F1 Score': result['f1_score']\n    })\n\ncomparison_df = pd.DataFrame(comparison_data)\ncomparison_df = comparison_df.sort_values('Accuracy', ascending=False)\nprint(comparison_df.to_string(index=False))\n\n# Find the best model\n# Get original name with (Count)\nbest_model_name = list(results.keys())[comparison_df.index[0]]\nbest_model = results[best_model_name]['model']\nprint(f\"\\nBest performing model: {comparison_df.iloc[0]['Model']}\")\nprint(f\"Best accuracy: {comparison_df.iloc[0]['Accuracy']:.4f}\")\nprint(f\"Best F1 score: {comparison_df.iloc[0]['F1 Score']:.4f}\")\n\n# Visualize results\nplt.figure(figsize=(12, 5))\n\n# Plot 1: Accuracy comparison\nplt.subplot(1, 2, 1)\nplt.bar(range(len(comparison_df)),\n        comparison_df['Accuracy'], color='lightcoral')\nplt.xlabel('Models')\nplt.ylabel('Accuracy')\nplt.title('CountVectorizer Model Accuracy Comparison')\nplt.xticks(range(len(comparison_df)),\n           comparison_df['Model'], rotation=45, ha='right')\nplt.ylim(0, 1)\n\n# Plot 2: F1 Score comparison\nplt.subplot(1, 2, 2)\nplt.bar(range(len(comparison_df)),\n        comparison_df['F1 Score'], color='lightgreen')\nplt.xlabel('Models')\nplt.ylabel('F1 Score')\nplt.title('CountVectorizer Model F1 Score Comparison')\nplt.xticks(range(len(comparison_df)),\n           comparison_df['Model'], rotation=45, ha='right')\nplt.ylim(0, 1)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create confusion matrix for the best CountVectorizer model\nbest_predictions = results[best_model_name]['predictions']\n\nplt.figure(figsize=(8, 6))\ncm = confusion_matrix(y_test, best_predictions)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Reds',\n            xticklabels=['Legitimate', 'Phishing'],\n            yticklabels=['Legitimate', 'Phishing'])\nplt.title(\n    f'Confusion Matrix - {comparison_df.iloc[0][\"Model\"]} (CountVectorizer)')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n# Print detailed classification report\nprint(\n    f\"\\nDetailed Classification Report for {comparison_df.iloc[0]['Model']}:\")\nprint(\"=\" * 60)\nprint(classification_report(y_test, best_predictions,\n      target_names=['Legitimate', 'Phishing']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CountVectorizer prediction functions\ndef predict_webpage_status(webpage_code, model=None, vectorizer=None):\n    \"\"\"\n    Predict if a webpage is phishing (1) or legitimate (0) based on its HTML code using CountVectorizer.\n\n    Parameters:\n    webpage_code (str): The HTML code of the webpage\n    model: The trained model to use for prediction (default: best model)\n    vectorizer: The CountVectorizer to use (default: loaded vectorizer)\n\n    Returns:\n    dict: Prediction result with probability scores\n    \"\"\"\n    if model is None:\n        if 'best_model' in globals():\n            model = best_model\n        else:\n            print(\"‚ùå No model available. Please train models first.\")\n            return None\n\n    if vectorizer is None:\n        if 'count_vectorizer' in globals():\n            vectorizer = count_vectorizer\n        else:\n            print(\"‚ùå No CountVectorizer available. Please load processed data first.\")\n            return None\n\n    try:\n        # Vectorize the input\n        webpage_vectorized = vectorizer.transform([webpage_code])\n\n        # Make prediction\n        prediction = model.predict(webpage_vectorized)[0]\n\n        # Get prediction probabilities\n        probabilities = model.predict_proba(webpage_vectorized)[0]\n\n        # Create result dictionary\n        result = {\n            'prediction': prediction,\n            'status': 'Phishing' if prediction == 1 else 'Legitimate',\n            'confidence': max(probabilities),\n            'probability_legitimate': probabilities[0],\n            'probability_phishing': probabilities[1],\n            'vectorizer_used': 'CountVectorizer'\n        }\n\n        return result\n\n    except Exception as e:\n        print(f\"‚ùå Error making prediction: {e}\")\n        return None\n\n\ndef load_model_for_prediction(model_name):\n    \"\"\"Load a specific trained CountVectorizer model for prediction\"\"\"\n    model_filename = f\"{model_name.replace(' ', '_').replace('(', '').replace(')', '').lower()}.joblib\"\n    model_path = os.path.join(ARTIFACT_DIR, model_filename)\n\n    if os.path.exists(model_path):\n        loaded_model = joblib.load(model_path)\n        print(f\"‚úÖ Loaded model: {model_name}\")\n        return loaded_model\n    else:\n        print(f\"‚ùå Model file not found: {model_path}\")\n        return None\n\n\ndef test_prediction_functions():\n    \"\"\"Test the prediction function with sample data using CountVectorizer\"\"\"\n\n    if 'results' not in globals() or not results:\n        print(\"‚ùå No trained models available. Please train models first.\")\n        return\n\n    print(f\"Using best model: {comparison_df.iloc[0]['Model']}\")\n    print(f\"Best accuracy: {comparison_df.iloc[0]['Accuracy']:.4f}\")\n\n    # Test with sample HTML codes\n    test_samples = [\n        \"<html><head><title>Google</title></head><body>Welcome to Google</body></html>\",\n        \"<html><head><title>Secure Bank Login</title></head><body><form>Enter password</form></body></html>\",\n        \"<html><script>window.location='phishing-site.com'</script></html>\",\n        \"<html><body>Click here to verify your account: <a href='fake-bank.com'>Verify</a></body></html>\",\n        \"<html><body>Urgent! Your account will be suspended. Click <a href='malicious-bank-site.com'>here</a></body></html>\"\n    ]\n\n    print(\"\\nüß™ TESTING COUNTVECTORIZER PREDICTION FUNCTION\")\n    print(\"=\" * 70)\n\n    for i, sample in enumerate(test_samples, 1):\n        result = predict_webpage_status(sample, best_model, count_vectorizer)\n        if result:\n            print(f\"\\nüìÑ Sample {i}: {sample[:60]}...\")\n            print(f\"   Prediction: {result['status']}\")\n            print(f\"   Confidence: {result['confidence']:.3f}\")\n            print(\n                f\"   Prob Legitimate: {result['probability_legitimate']:.3f}\")\n            print(f\"   Prob Phishing: {result['probability_phishing']:.3f}\")\n        else:\n            print(f\"\\nüìÑ Sample {i}: ‚ùå Prediction failed\")\n\n# Note: Run test_prediction_functions() after training models","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# üéØ CountVectorizer Batch Processing Workflow Summary\nprint(\"\"\"\nüìä COUNTVECTORIZER BATCH PROCESSING WORKFLOW FOR PHISHING DETECTION\n================================================================\n\nThis notebook implements an efficient batch processing approach for handling large datasets using CountVectorizer:\n\nüîÑ WORKFLOW STEPS:\n1. ‚úÖ Load dataset in chunks (2000 rows at a time)\n2. ‚úÖ Analyze dataset structure and statistics \n3. ‚úÖ Create stratified train-test split\n4. ‚úÖ Vectorize data in batches using CountVectorizer\n5. ‚úÖ Save vectorized data and vectorizer to disk\n6. ‚úÖ Train multiple models on processed data\n7. ‚úÖ Save trained models for future use\n8. ‚úÖ Evaluate and compare model performance\n\nüíæ SAVED ARTIFACTS (in /kaggle/working/count/):\n- count_vectorizer.joblib          (CountVectorizer)\n- X_train_count.npz               (Training features)\n- X_test_count.npz                (Test features) \n- y_train_count.npy               (Training labels)\n- y_test_count.npy                (Test labels)\n- [model_name].joblib             (Trained models)\n\nüöÄ MEMORY MANAGEMENT:\n- Processes data in 2000-row batches\n- Saves intermediate results to disk\n- Cleans up memory after each batch\n- Monitors memory usage throughout\n\nüìà MODELS TRAINED:\n- Random Forest\n- XGBoost  \n- LightGBM\n- Extra Trees\n- Naive Bayes\n- Logistic Regression\n\nüéØ USAGE:\n1. Run all cells in sequence to process data and train models\n2. Use load_count_processed_data() to reload saved data\n3. Use predict_webpage_status() for new predictions\n4. Use test_prediction_functions() to test the prediction pipeline\n\nüí° COUNTVECTORIZER BENEFITS:\n- Fast and simple text vectorization\n- Preserves exact word counts\n- Less computationally intensive than TF-IDF\n- Good baseline for text classification\n- Handles large datasets efficiently in batches\n\"\"\")\n\n# Print current status\nprint(f\"\\nüìç CURRENT STATUS:\")\nprint(f\"Working directory: {os.getcwd()}\")\nprint(f\"Artifact directory: {ARTIFACT_DIR}\")\nprint(f\"Dataset path: {dataset_path}\")\n\n# Check if processed data exists\nif os.path.exists(os.path.join(ARTIFACT_DIR, \"count_vectorizer.joblib\")):\n    print(\"‚úÖ CountVectorizer processed data available\")\nelse:\n    print(\"‚è≥ CountVectorizer processed data not yet created - run processing cells first\")\n\n# Memory status\nprint_memory_usage(\"(current)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Phishing URL Detection","metadata":{}},{"cell_type":"code","source":"# Check if we're running locally or on Kaggle for URL dataset\nif os.path.exists(\"/home/iqbal/Programming/ML/project/dataset/new_data_urls.csv\"):\n    url_dataset_path = \"/home/iqbal/Programming/ML/project/dataset/new_data_urls.csv\"\nelse:\n    url_dataset_path = \"/kaggle/input/phising-website-url-dataset/new_data_urls.csv\"\n\nurl_dataSet = pd.read_csv(url_dataset_path)\nurl_dataSet.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare features and target for URL dataset\nfrom sklearn.model_selection import train_test_split\n\nurl_X = url_dataSet['url']\nurl_y = url_dataSet['status']\n\nprint(f\"URL dataset shape: {url_dataSet.shape}\")\nprint(f\"Class distribution:\\n{url_y.value_counts()}\")\n\n# Split into train/test sets\nurl_X_train, url_X_test, url_y_train, url_y_test = train_test_split(\n    url_X, url_y, test_size=0.2, random_state=42, stratify=url_y\n)\nprint(f\"Train size: {len(url_X_train)}, Test size: {len(url_X_test)}\")\n\n# Create pipelines for URL classification\npipelines = {\n    'CountVectorizer + Random Forest': Pipeline([\n        ('vectorizer', CountVectorizer(max_features=5000, stop_words='english')),\n        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n    ]),\n    'CountVectorizer + XGBoost': Pipeline([\n        ('vectorizer', CountVectorizer(max_features=5000, stop_words='english')),\n        ('classifier', xgb.XGBClassifier(random_state=42, verbosity=0))\n    ]),\n    'CountVectorizer + Naive Bayes': Pipeline([\n        ('vectorizer', CountVectorizer(max_features=5000, stop_words='english')),\n        ('classifier', MultinomialNB())\n    ]),\n    'CountVectorizer + Logistic Regression': Pipeline([\n        ('vectorizer', CountVectorizer(max_features=5000, stop_words='english')),\n        ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n    ])\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n\n\nurl_results = {}\nprint(\"Training and evaluating URL models...\")\nfor name, pipeline in pipelines.items():\n    print(f\"\\nTraining {name}...\")\n    try:\n        pipeline.fit(url_X_train, url_y_train)\n        y_pred = pipeline.predict(url_X_test)\n        acc = accuracy_score(url_y_test, y_pred)\n        f1 = f1_score(url_y_test, y_pred)\n        url_results[name] = {'accuracy': acc, 'f1_score': f1,\n                             'predictions': y_pred, 'model': pipeline}\n        print(f\"Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n    except Exception as e:\n        print(f\"Error training {name}: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compare URL model performances\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncomparison_url = []\nfor name, result in url_results.items():\n    comparison_url.append({\n        'Model': name,\n        'Accuracy': result['accuracy'],\n        'F1 Score': result['f1_score']\n    })\n\ncomparison_url_df = pd.DataFrame(\n    comparison_url).sort_values('Accuracy', ascending=False)\nprint(comparison_url_df.to_string(index=False))\n\nbest_url_model_name = comparison_url_df.iloc[0]['Model']\nbest_url_model = url_results[best_url_model_name]['model']\nprint(f\"\\nBest URL model: {best_url_model_name}\")\nprint(f\"Accuracy: {comparison_url_df.iloc[0]['Accuracy']:.4f}\")\n\n# Visualize accuracy and F1 score\nplt.figure(figsize=(10, 4))\nplt.bar(comparison_url_df['Model'], comparison_url_df['Accuracy'],\n        color='skyblue', label='Accuracy')\nplt.bar(comparison_url_df['Model'], comparison_url_df['F1 Score'],\n        color='lightcoral', alpha=0.7, label='F1 Score')\nplt.xticks(rotation=45, ha='right')\nplt.ylabel('Score')\nplt.title('Phishing URL Model Performance')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Confusion matrix for best model\ny_pred_best = url_results[best_url_model_name]['predictions']\ncm = confusion_matrix(url_y_test, y_pred_best)\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\n            'Legitimate', 'Phishing'], yticklabels=['Legitimate', 'Phishing'])\nplt.title(f'Confusion Matrix - {best_url_model_name}')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a function to predict URL status\ndef predict_url_status(url, model=None):\n    \"\"\"\n    Predict if a URL is phishing (1) or legitimate (0) based on its text.\n\n    Parameters:\n    url (str): The URL to predict\n    model: The trained model to use for prediction (default: best model)\n\n    Returns:\n    dict: Prediction result with probability scores\n    \"\"\"\n    if model is None:\n        model = best_url_model\n\n    # Make prediction\n    prediction = model.predict([url])[0]\n\n    # Get prediction probabilities\n    probabilities = model.predict_proba([url])[0]\n\n    # Create result dictionary\n    result = {\n        'prediction': prediction,\n        'status': 'Legitimate' if prediction == 1 else 'Phishing',\n        'confidence': max(probabilities),\n        'probability_legitimate': probabilities[0],\n        'probability_phishing': probabilities[1]\n    }\n\n    return result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"url_sample = [\"google.com\", \"facebook.com\", \"phishing-test.com\",\n              \"example.com\", \"malicious-site.com\", 'facebook-test.com']\n\nprint(\"\\nTesting URL prediction function:\")\nfor url in url_sample:\n    result = predict_url_status(url)\n    print(f\"URL: {url} | Prediction: {result['status']} | \"\n          f\"Confidence: {result['confidence']:.4f} | \"\n          f\"Prob Legitimate: {result['probability_legitimate']:.4f} | \"\n          f\"Prob Phishing: {result['probability_phishing']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}