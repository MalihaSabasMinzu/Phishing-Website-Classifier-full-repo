{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12693261,"sourceType":"datasetVersion","datasetId":8021783},{"sourceId":12742804,"sourceType":"datasetVersion","datasetId":8055177},{"sourceId":12751295,"sourceType":"datasetVersion","datasetId":8060741}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predict from web_code","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.preprocessing import StandardScaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:37:43.319204Z","iopub.execute_input":"2025-08-09T17:37:43.320012Z","iopub.status.idle":"2025-08-09T17:37:52.206553Z","shell.execute_reply.started":"2025-08-09T17:37:43.319962Z","shell.execute_reply":"2025-08-09T17:37:52.205494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = pd.read_csv(\n    \"/kaggle/input/phishing-website-webcode-dataset/phishing_complete_dataset.csv\",\n    sep=\",\",\n    quotechar='\"'\n)\n\ndataset.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:37:52.208489Z","iopub.execute_input":"2025-08-09T17:37:52.209431Z","iopub.status.idle":"2025-08-09T17:41:18.021983Z","shell.execute_reply.started":"2025-08-09T17:37:52.209397Z","shell.execute_reply":"2025-08-09T17:41:18.021016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check dataset shape and class distribution\nprint(f\"Dataset shape: {dataset.shape}\")\nprint(f\"\\nClass distribution:\")\nprint(dataset['result'].value_counts())\nprint(f\"\\nClass distribution (percentages):\")\nprint(dataset['result'].value_counts(normalize=True) * 100)\n\n# Check for missing values\nprint(f\"\\nMissing values:\")\nprint(dataset.isnull().sum())\n\n# Check length of webpage_code\ndataset['code_length'] = dataset['webpage_code'].str.len()\nprint(f\"\\nWebpage code length statistics:\")\nprint(dataset['code_length'].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:41:18.022849Z","iopub.execute_input":"2025-08-09T17:41:18.023164Z","iopub.status.idle":"2025-08-09T17:41:18.160446Z","shell.execute_reply.started":"2025-08-09T17:41:18.023140Z","shell.execute_reply":"2025-08-09T17:41:18.159576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare the data\n# Features (X) = webpage_code, Target (y) = result\nX = dataset['webpage_code']\ny = dataset['result']\n\nprint(f\"Features shape: {X.shape}\")\nprint(f\"Target shape: {y.shape}\")\nprint(f\"Target distribution:\\n{y.value_counts()}\")\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\ndel X\ndel y\ndel dataset\n\nprint(f\"\\nTraining set size: {len(X_train)}\")\nprint(f\"Test set size: {len(X_test)}\")\nprint(f\"Training target distribution:\\n{y_train.value_counts()}\")\nprint(f\"Test target distribution:\\n{y_test.value_counts()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:41:18.161788Z","iopub.execute_input":"2025-08-09T17:41:18.162046Z","iopub.status.idle":"2025-08-09T17:41:18.219485Z","shell.execute_reply.started":"2025-08-09T17:41:18.162026Z","shell.execute_reply":"2025-08-09T17:41:18.218595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Vectorize once for both TF-IDF and CountVectorizer, then release raw data\ntfidf_vectorizer = TfidfVectorizer(\n    max_features=5000, stop_words='english', ngram_range=(1, 2))\n# count_vectorizer = CountVectorizer(\n#     max_features=5000, stop_words='english', ngram_range=(1, 2))\n\n# Fit and transform training data\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n# X_train_count = count_vectorizer.fit_transform(X_train)\n# X_test_count = count_vectorizer.transform(X_test)\n\n# Release raw text data from memory\ndel X_train\ndel X_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:41:18.221575Z","iopub.execute_input":"2025-08-09T17:41:18.221900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport joblib\nfrom scipy.sparse import save_npz\n\nARTIFACT_DIR = \"/kaggle/working/\"\n\njoblib.dump(tfidf_vectorizer, os.path.join(\n    ARTIFACT_DIR, \"tfidf_vectorizer.joblib\"))\n\nsave_npz(os.path.join(ARTIFACT_DIR, \"X_train_tfidf.npz\"), X_train_tfidf)\nsave_npz(os.path.join(ARTIFACT_DIR, \"X_test_tfidf.npz\"), X_test_tfidf)\n\ny_train_np = y_train.to_numpy() if hasattr(\n    y_train, \"to_numpy\") else np.asarray(y_train)\ny_test_np = y_test.to_numpy() if hasattr(\n    y_test, \"to_numpy\") else np.asarray(y_test)\nnp.save(os.path.join(ARTIFACT_DIR, \"y_train.npy\"), y_train_np)\nnp.save(os.path.join(ARTIFACT_DIR, \"y_test.npy\"), y_test_np)\n\n# joblib.dump(count_vectorizer, os.path.join(\n#     ARTIFACT_DIR, \"count_vectorizer.joblib\"))\n\n# save_npz(os.path.join(ARTIFACT_DIR, \"X_train_count.npz\"), X_train_count)\n# save_npz(os.path.join(ARTIFACT_DIR, \"X_test_count.npz\"), X_test_count)\n\n# np.save(os.path.join(ARTIFACT_DIR, \"y_train.npy\"), y_train_np)\n# np.save(os.path.join(ARTIFACT_DIR, \"y_test.npy\"), y_test_np)\n\nprint(f\"Saved TF-IDF vectorizer and datasets to '{ARTIFACT_DIR}'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = {}\n\nclassifiers_tfidf = {\n    'Random Forest (TF-IDF)': RandomForestClassifier(n_estimators=100, random_state=42),\n    'XGBoost (TF-IDF)': xgb.XGBClassifier(random_state=42, verbosity=0),\n    'LightGBM (TF-IDF)': lgb.LGBMClassifier(random_state=42, verbose=-1),\n    'Extra Trees (TF-IDF)': ExtraTreesClassifier(n_estimators=100, random_state=42)\n}\n\n# classifiers_count = {\n#     'Random Forest (Count)': RandomForestClassifier(n_estimators=100, random_state=42),\n#     'XGBoost (Count)': xgb.XGBClassifier(random_state=42, verbosity=0),\n#     'LightGBM (Count)': lgb.LGBMClassifier(random_state=42, verbose=-1),\n#     'Extra Trees (Count)': ExtraTreesClassifier(n_estimators=100, random_state=42)\n# }\n\nprint(\"Training and evaluating models...\")\nprint(\"=\" * 50)\n\n# Train TF-IDF models\nfor name, clf in classifiers_tfidf.items():\n    print(f\"\\nTraining {name}...\")\n    try:\n        clf.fit(X_train_tfidf, y_train)\n        y_pred = clf.predict(X_test_tfidf)\n        accuracy = accuracy_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred)\n        results[name] = {\n            'accuracy': accuracy,\n            'f1_score': f1,\n            'predictions': y_pred,\n            'model': clf\n        }\n    except Exception as e:\n        print(f\"Error training {name}: {e}\")\n\n# Train CountVectorizer models\n# for name, clf in classifiers_count.items():\n#     print(f\"\\nTraining {name}...\")\n#     try:\n#         clf.fit(X_train_count, y_train)\n#         y_pred = clf.predict(X_test_count)\n#         accuracy = accuracy_score(y_test, y_pred)\n#         f1 = f1_score(y_test, y_pred)\n#         results[name] = {\n#             'accuracy': accuracy,\n#             'f1_score': f1,\n#             'predictions': y_pred,\n#             'model': clf\n#         }\n#     except Exception as e:\n#         print(f\"Error training {name}: {e}\")\n\nprint(f\"\\n{'='*50}\")\nprint(\"Training completed!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compare model performances\nprint(\"Model Performance Comparison:\")\nprint(\"=\" * 60)\n\n# Create comparison dataframe\ncomparison_data = []\nfor name, result in results.items():\n    comparison_data.append({\n        'Model': name,\n        'Accuracy': result['accuracy'],\n        'F1 Score': result['f1_score']\n    })\n\ncomparison_df = pd.DataFrame(comparison_data)\ncomparison_df = comparison_df.sort_values('Accuracy', ascending=False)\nprint(comparison_df.to_string(index=False))\n\n# Find the best model\nbest_model_name = comparison_df.iloc[0]['Model']\nbest_model = results[best_model_name]['model']\nprint(f\"\\nBest performing model: {best_model_name}\")\nprint(f\"Best accuracy: {comparison_df.iloc[0]['Accuracy']:.4f}\")\n\n# Visualize results\nplt.figure(figsize=(12, 5))\n\n# Plot 1: Accuracy comparison\nplt.subplot(1, 2, 1)\nplt.bar(range(len(comparison_df)), comparison_df['Accuracy'], color='skyblue')\nplt.xlabel('Models')\nplt.ylabel('Accuracy')\nplt.title('Model Accuracy Comparison')\nplt.xticks(range(len(comparison_df)),\n           comparison_df['Model'], rotation=45, ha='right')\nplt.ylim(0, 1)\n\n# Plot 2: F1 Score comparison\nplt.subplot(1, 2, 2)\nplt.bar(range(len(comparison_df)),\n        comparison_df['F1 Score'], color='lightcoral')\nplt.xlabel('Models')\nplt.ylabel('F1 Score')\nplt.title('Model F1 Score Comparison')\nplt.xticks(range(len(comparison_df)),\n           comparison_df['Model'], rotation=45, ha='right')\nplt.ylim(0, 1)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create confusion matrix for the best model\nbest_predictions = results[best_model_name]['predictions']\n\nplt.figure(figsize=(8, 6))\ncm = confusion_matrix(y_test, best_predictions)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Legitimate', 'Phishing'],\n            yticklabels=['Legitimate', 'Phishing'])\nplt.title(f'Confusion Matrix - {best_model_name}')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to predict if a webpage is phishing or legitimate\ndef predict_webpage_status(webpage_code, model=None):\n    \"\"\"\n    Predict if a webpage is phishing (1) or legitimate (0) based on its HTML code.\n\n    Parameters:\n    webpage_code (str): The HTML code of the webpage\n    model: The trained model to use for prediction (default: best model)\n\n    Returns:\n    dict: Prediction result with probability scores\n    \"\"\"\n    if model is None:\n        model = best_model\n\n    # Make prediction\n    prediction = model.predict([webpage_code])[0]\n\n    # Get prediction probabilities\n    probabilities = model.predict_proba([webpage_code])[0]\n\n    # Create result dictionary\n    result = {\n        'prediction': prediction,\n        'status': 'Phishing' if prediction == 1 else 'Legitimate',\n        'confidence': max(probabilities),\n        'probability_legitimate': probabilities[0],\n        'probability_phishing': probabilities[1]\n    }\n\n    return result\n\n\n# Test the function with a sample from the test set\n# sample_index = 22\n# sample_code = X_train_tfidf.iloc[sample_index]\n# actual_label = y_test.iloc[sample_index]\n\n# prediction_result = predict_webpage_status(sample_code)\n\n# print(\"Testing the prediction function:\")\n# print(\"=\" * 40)\n# print(\n#     f\"Actual label: {actual_label} ({'Phishing' if actual_label == 1 else 'Legitimate'})\")\n# print(f\"Predicted: {prediction_result['status']}\")\n# print(f\"Confidence: {prediction_result['confidence']:.4f}\")\n# print(\n#     f\"Probability Legitimate: {prediction_result['probability_legitimate']:.4f}\")\n# print(f\"Probability Phishing: {prediction_result['probability_phishing']:.4f}\")\n\n# # Test with a few more samples\n# print(f\"\\nTesting with 5 random samples:\")\n# print(\"=\" * 50)\n# for i in range(5):\n#     sample_code = X_train_tfidf.iloc[i]\n#     actual_label = y_test.iloc[i]\n#     prediction_result = predict_webpage_status(sample_code)\n\n#     correct = \"✓\" if prediction_result['prediction'] == actual_label else \"✗\"\n#     print(f\"Sample {i+1}: Actual: {actual_label}, Predicted: {prediction_result['prediction']}, \"\n#           f\"Confidence: {prediction_result['confidence']:.3f} {correct}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Phishing URL Detection","metadata":{}},{"cell_type":"code","source":"url_dataSet = pd.read_csv(\n    \"/kaggle/input/phising-website-url-dataset/new_data_urls.csv\"\n)\nurl_dataSet.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare features and target for URL dataset\nfrom sklearn.model_selection import train_test_split\nurl_X = url_dataSet['url']\n# fallback if column name differs\nurl_y = url_dataSet['status']\n\nprint(f\"URL dataset shape: {url_dataSet.shape}\")\nprint(f\"Class distribution:\\n{url_y.value_counts()}\")\n\n# Split into train/test sets\nurl_X_train, url_X_test, url_y_train, url_y_test = train_test_split(\n    url_X, url_y, test_size=0.2, random_state=42, stratify=url_y\n)\nprint(f\"Train size: {len(url_X_train)}, Test size: {len(url_X_test)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n\n\nurl_results = {}\nprint(\"Training and evaluating URL models...\")\nfor name, pipeline in pipelines.items():\n    print(f\"\\nTraining {name}...\")\n    try:\n        pipeline.fit(url_X_train, url_y_train)\n        y_pred = pipeline.predict(url_X_test)\n        acc = accuracy_score(url_y_test, y_pred)\n        f1 = f1_score(url_y_test, y_pred)\n        url_results[name] = {'accuracy': acc, 'f1_score': f1,\n                             'predictions': y_pred, 'model': pipeline}\n        print(f\"Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n    except Exception as e:\n        print(f\"Error training {name}: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compare URL model performances\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncomparison_url = []\nfor name, result in url_results.items():\n    comparison_url.append({\n        'Model': name,\n        'Accuracy': result['accuracy'],\n        'F1 Score': result['f1_score']\n    })\n\ncomparison_url_df = pd.DataFrame(\n    comparison_url).sort_values('Accuracy', ascending=False)\nprint(comparison_url_df.to_string(index=False))\n\nbest_url_model_name = comparison_url_df.iloc[0]['Model']\nbest_url_model = url_results[best_url_model_name]['model']\nprint(f\"\\nBest URL model: {best_url_model_name}\")\nprint(f\"Accuracy: {comparison_url_df.iloc[0]['Accuracy']:.4f}\")\n\n# Visualize accuracy and F1 score\nplt.figure(figsize=(10, 4))\nplt.bar(comparison_url_df['Model'], comparison_url_df['Accuracy'],\n        color='skyblue', label='Accuracy')\nplt.bar(comparison_url_df['Model'], comparison_url_df['F1 Score'],\n        color='lightcoral', alpha=0.7, label='F1 Score')\nplt.xticks(rotation=45, ha='right')\nplt.ylabel('Score')\nplt.title('Phishing URL Model Performance')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Confusion matrix for best model\ny_pred_best = url_results[best_url_model_name]['predictions']\ncm = confusion_matrix(url_y_test, y_pred_best)\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\n            'Legitimate', 'Phishing'], yticklabels=['Legitimate', 'Phishing'])\nplt.title(f'Confusion Matrix - {best_url_model_name}')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a function to predict URL status\ndef predict_url_status(url, model=None):\n    \"\"\"\n    Predict if a URL is phishing (1) or legitimate (0) based on its text.\n\n    Parameters:\n    url (str): The URL to predict\n    model: The trained model to use for prediction (default: best model)\n\n    Returns:\n    dict: Prediction result with probability scores\n    \"\"\"\n    if model is None:\n        model = best_url_model\n\n    # Make prediction\n    prediction = model.predict([url])[0]\n\n    # Get prediction probabilities\n    probabilities = model.predict_proba([url])[0]\n\n    # Create result dictionary\n    result = {\n        'prediction': prediction,\n        'status': 'Legitimate' if prediction == 1 else 'Phishing',\n        'confidence': max(probabilities),\n        'probability_legitimate': probabilities[0],\n        'probability_phishing': probabilities[1]\n    }\n\n    return result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"url_sample = [\"google.com\", \"facebook.com\", \"phishing-test.com\",\n              \"example.com\", \"malicious-site.com\", 'facebook-test.com']\n\nprint(\"\\nTesting URL prediction function:\")\nfor url in url_sample:\n    result = predict_url_status(url)\n    print(f\"URL: {url} | Prediction: {result['status']} | \"\n          f\"Confidence: {result['confidence']:.4f} | \"\n          f\"Prob Legitimate: {result['probability_legitimate']:.4f} | \"\n          f\"Prob Phishing: {result['probability_phishing']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}