{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12693261,"sourceType":"datasetVersion","datasetId":8021783},{"sourceId":12742804,"sourceType":"datasetVersion","datasetId":8055177},{"sourceId":12751295,"sourceType":"datasetVersion","datasetId":8060741}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Phishing Website Detection using CountVectorizer","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2025-08-10T12:21:42.746434Z","iopub.execute_input":"2025-08-10T12:21:42.747100Z","iopub.status.idle":"2025-08-10T12:21:45.555119Z","shell.execute_reply.started":"2025-08-10T12:21:42.747068Z","shell.execute_reply":"2025-08-10T12:21:45.554139Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import psutil\nimport gc\n\n\ndef get_memory_usage():\n    \"\"\"Get current memory usage\"\"\"\n    process = psutil.Process()\n    memory_info = process.memory_info()\n    memory_mb = memory_info.rss / 1024 / 1024\n    return memory_mb\n\n\ndef print_memory_usage(label=\"\"):\n    \"\"\"Print current memory usage with optional label\"\"\"\n    memory_mb = get_memory_usage()\n    print(f\"Memory usage {label}: {memory_mb:.2f} MB\")\n\n\ndef cleanup_memory():\n    \"\"\"Force garbage collection to free memory\"\"\"\n    gc.collect()\n    print(\"Memory cleanup completed\")\n\n\n# Print initial memory usage\nprint_memory_usage(\"(initial)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T12:21:45.556836Z","iopub.execute_input":"2025-08-10T12:21:45.557450Z","iopub.status.idle":"2025-08-10T12:21:45.564816Z","shell.execute_reply.started":"2025-08-10T12:21:45.557424Z","shell.execute_reply":"2025-08-10T12:21:45.563846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport joblib\nimport numpy as np\nfrom scipy.sparse import save_npz, load_npz, vstack\nimport gc\n\n\ndataset_path = \"/kaggle/input/phishing-website-webcode-dataset/phishing_complete_dataset.csv\"\nARTIFACT_DIR = \"/kaggle/working/count/\"\n\n# Create artifacts directory if it doesn't exist\nos.makedirs(ARTIFACT_DIR, exist_ok=True)\n\n# First, let's check the total number of rows in the dataset\ntotal_rows = sum(1 for line in open(dataset_path)) - 1  # subtract 1 for header\nprint(f\"Total rows in dataset: {total_rows}\")\n\n# Read a small sample first to understand the data structure\nsample_df = pd.read_csv(dataset_path, nrows=5)\nprint(f\"Dataset columns: {sample_df.columns.tolist()}\")\nprint(f\"Sample data:\")\nprint(sample_df.head())","metadata":{"execution":{"iopub.status.busy":"2025-08-10T12:21:45.565822Z","iopub.execute_input":"2025-08-10T12:21:45.566080Z","iopub.status.idle":"2025-08-10T12:24:11.629101Z","shell.execute_reply.started":"2025-08-10T12:21:45.566052Z","shell.execute_reply":"2025-08-10T12:24:11.627980Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Batch processing configuration\nBATCH_SIZE = 2000\nCHUNK_SIZE = 2000\n\n\ndef process_dataset_in_batches():\n    \"\"\"Process dataset in batches to manage memory efficiently\"\"\"\n\n    # Initialize variables to track dataset statistics\n    total_processed = 0\n    class_counts = {}\n    code_lengths = []\n\n    print(\"Processing dataset in batches to analyze structure...\")\n\n    # Process dataset in chunks for analysis\n    chunk_iter = pd.read_csv(dataset_path, chunksize=CHUNK_SIZE)\n\n    for i, chunk in enumerate(chunk_iter):\n        print(\n            f\"Processing chunk {i+1}, rows {total_processed+1} to {total_processed+len(chunk)}\")\n\n        # Update class distribution\n        chunk_classes = chunk['result'].value_counts()\n        for class_label, count in chunk_classes.items():\n            class_counts[class_label] = class_counts.get(\n                class_label, 0) + count\n\n        # Sample some code lengths (to avoid memory issues)\n        if len(code_lengths) < 10000:  # Only sample first 10k for statistics\n            chunk_lengths = chunk['webpage_code'].str.len()\n            code_lengths.extend(chunk_lengths.tolist())\n\n        total_processed += len(chunk)\n\n        # Clear chunk from memory\n        del chunk\n        gc.collect()\n\n        if total_processed >= 10000:  # Limit analysis to first 10k rows for speed\n            break\n\n    print(f\"\\nDataset Analysis (first {total_processed} rows):\")\n    print(f\"Total processed: {total_processed}\")\n    print(f\"Class distribution: {class_counts}\")\n\n    if class_counts:\n        total_samples = sum(class_counts.values())\n        for class_label, count in class_counts.items():\n            percentage = (count / total_samples) * 100\n            print(f\"Class {class_label}: {count} ({percentage:.2f}%)\")\n\n    if code_lengths:\n        code_lengths_array = np.array(code_lengths)\n        print(f\"\\nWebpage code length statistics:\")\n        print(f\"Mean: {code_lengths_array.mean():.2f}\")\n        print(f\"Std: {code_lengths_array.std():.2f}\")\n        print(f\"Min: {code_lengths_array.min()}\")\n        print(f\"Max: {code_lengths_array.max()}\")\n        print(f\"Median: {np.median(code_lengths_array):.2f}\")\n\n\n# Run the analysis\nprocess_dataset_in_batches()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T12:24:11.630410Z","iopub.execute_input":"2025-08-10T12:24:11.631295Z","iopub.status.idle":"2025-08-10T12:24:28.458732Z","shell.execute_reply.started":"2025-08-10T12:24:11.631268Z","shell.execute_reply":"2025-08-10T12:24:28.457920Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_train_test_split_batched():\n    \"\"\"Create train-test split while processing dataset in batches\"\"\"\n\n    print(\"Creating train-test split with batched processing...\")\n\n    # First pass: collect all data for proper stratified split\n    all_X = []\n    all_y = []\n\n    chunk_iter = pd.read_csv(dataset_path, chunksize=CHUNK_SIZE)\n\n    for i, chunk in enumerate(chunk_iter):\n        print(f\"Reading chunk {i+1} for train-test split...\")\n        all_X.extend(chunk['webpage_code'].tolist())\n        all_y.extend(chunk['result'].tolist())\n        del chunk\n        gc.collect()\n\n    print(f\"Total samples loaded: {len(all_X)}\")\n\n    # Convert to pandas Series for train_test_split\n    X_series = pd.Series(all_X)\n    y_series = pd.Series(all_y)\n\n    # Create stratified train-test split\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_series, y_series, test_size=0.2, random_state=42, stratify=y_series\n    )\n\n    print(f\"Training set size: {len(X_train)}\")\n    print(f\"Test set size: {len(X_test)}\")\n    print(f\"Training target distribution:\\n{y_train.value_counts()}\")\n    print(f\"Test target distribution:\\n{y_test.value_counts()}\")\n\n    # Clear original data from memory\n    del all_X, all_y, X_series, y_series\n    gc.collect()\n\n    return X_train, X_test, y_train, y_test\n\n\n# Create the train-test split\nX_train, X_test, y_train, y_test = create_train_test_split_batched()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T12:24:28.461024Z","iopub.execute_input":"2025-08-10T12:24:28.461320Z","iopub.status.idle":"2025-08-10T12:27:04.112125Z","shell.execute_reply.started":"2025-08-10T12:24:28.461282Z","shell.execute_reply":"2025-08-10T12:27:04.111295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load previously processed CountVectorizer data if needed\n# Uncomment the lines below to load previously saved CountVectorizer data instead of processing\n# loaded_count_data = load_count_processed_data()\n# if loaded_count_data:\n#     count_vectorizer = loaded_count_data['vectorizer']\n#     X_train_count = loaded_count_data['X_train_count']\n#     X_test_count = loaded_count_data['X_test_count']\n#     y_train = loaded_count_data['y_train']\n#     y_test = loaded_count_data['y_test']\n#     print(\"✅ Loaded previously processed CountVectorizer data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T12:27:04.113095Z","iopub.execute_input":"2025-08-10T12:27:04.113367Z","iopub.status.idle":"2025-08-10T12:27:04.117685Z","shell.execute_reply.started":"2025-08-10T12:27:04.113339Z","shell.execute_reply":"2025-08-10T12:27:04.116865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def vectorize_data_count_in_batches(X_train, X_test, y_train, y_test):\n    \"\"\"Vectorize training and test data using CountVectorizer in batches to manage memory\"\"\"\n\n    print(\"Starting CountVectorizer batch processing...\")\n    print(f\"✓ Count artifacts will be saved to: {ARTIFACT_DIR}\")\n\n    # Initialize Count vectorizer\n    count_vectorizer = CountVectorizer(\n        max_features=5000,\n        stop_words='english',\n        ngram_range=(1, 2)\n    )\n\n    # Convert to lists for batch processing\n    X_train_list = X_train.tolist() if hasattr(\n        X_train, 'tolist') else list(X_train)\n    X_test_list = X_test.tolist() if hasattr(X_test, 'tolist') else list(X_test)\n\n    # Fit vectorizer on training data in batches\n    print(\"Fitting CountVectorizer on training data...\")\n\n    # For fitting, we need to process all training data\n    # We'll do this in batches but accumulate the vocabulary\n    train_batches = [X_train_list[i:i+BATCH_SIZE]\n                     for i in range(0, len(X_train_list), BATCH_SIZE)]\n\n    print(f\"Processing {len(train_batches)} training batches...\")\n\n    # Fit on the first few batches to establish vocabulary\n    # Use up to 10k samples for fitting\n    sample_size = min(10000, len(X_train_list))\n    sample_data = X_train_list[:sample_size]\n    count_vectorizer.fit(sample_data)\n\n    print(\"CountVectorizer fitted. Now transforming training data in batches...\")\n\n    # Transform training data in batches and save\n    X_train_count_batches = []\n    for i, batch in enumerate(train_batches):\n        print(f\"Transforming training batch {i+1}/{len(train_batches)}\")\n        batch_count = count_vectorizer.transform(batch)\n        X_train_count_batches.append(batch_count)\n\n        # Save batch to disk to free memory\n        batch_filename = os.path.join(\n            ARTIFACT_DIR, f\"X_train_count_batch_{i}.npz\")\n        save_npz(batch_filename, batch_count)\n\n        # Clear batch from memory\n        del batch_count\n        gc.collect()\n\n    print(\"Training data CountVectorization completed. Now processing test data...\")\n\n    # Transform test data in batches\n    test_batches = [X_test_list[i:i+BATCH_SIZE]\n                    for i in range(0, len(X_test_list), BATCH_SIZE)]\n    X_test_count_batches = []\n\n    for i, batch in enumerate(test_batches):\n        print(f\"Transforming test batch {i+1}/{len(test_batches)}\")\n        batch_count = count_vectorizer.transform(batch)\n        X_test_count_batches.append(batch_count)\n\n        # Save batch to disk\n        batch_filename = os.path.join(\n            ARTIFACT_DIR, f\"X_test_count_batch_{i}.npz\")\n        save_npz(batch_filename, batch_count)\n\n        # Clear batch from memory\n        del batch_count\n        gc.collect()\n\n    print(\"Combining all training batches...\")\n    # Load and combine all training batches\n    X_train_count = None\n    for i in range(len(train_batches)):\n        batch_filename = os.path.join(\n            ARTIFACT_DIR, f\"X_train_count_batch_{i}.npz\")\n        batch_data = load_npz(batch_filename)\n\n        if X_train_count is None:\n            X_train_count = batch_data\n        else:\n            X_train_count = vstack([X_train_count, batch_data])\n\n        # Clean up temporary file\n        os.remove(batch_filename)\n\n    print(\"Combining all test batches...\")\n    # Load and combine all test batches\n    X_test_count = None\n    for i in range(len(test_batches)):\n        batch_filename = os.path.join(\n            ARTIFACT_DIR, f\"X_test_count_batch_{i}.npz\")\n        batch_data = load_npz(batch_filename)\n\n        if X_test_count is None:\n            X_test_count = batch_data\n        else:\n            X_test_count = vstack([X_test_count, batch_data])\n\n        # Clean up temporary file\n        os.remove(batch_filename)\n\n    print(f\"CountVectorization completed!\")\n    print(f\"Training data shape: {X_train_count.shape}\")\n    print(f\"Test data shape: {X_test_count.shape}\")\n\n    # Clear original text data from memory\n    del X_train_list, X_test_list\n    gc.collect()\n\n    return count_vectorizer, X_train_count, X_test_count","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T12:27:04.118670Z","iopub.execute_input":"2025-08-10T12:27:04.118913Z","iopub.status.idle":"2025-08-10T12:27:04.150472Z","shell.execute_reply.started":"2025-08-10T12:27:04.118890Z","shell.execute_reply":"2025-08-10T12:27:04.149387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Process data with CountVectorizer in batches\nprint(\"Starting CountVectorizer batch processing...\")\nprint_memory_usage(\"(before vectorization)\")\n\n    # Call the CountVectorizer batch processing function\n    count_vectorizer, X_train_count, X_test_count = vectorize_data_count_in_batches(\n        X_train, X_test, y_train, y_test)\n    \n    # Clear original text data\n    del X_train, X_test\n    gc.collect()\n    \n    print_memory_usage(\"(after vectorization and cleanup)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T12:27:04.151557Z","iopub.execute_input":"2025-08-10T12:27:04.151882Z","iopub.status.idle":"2025-08-10T13:01:48.620178Z","shell.execute_reply.started":"2025-08-10T12:27:04.151844Z","shell.execute_reply":"2025-08-10T13:01:48.619363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_count_processed_data(count_vectorizer, X_train_count, X_test_count, y_train, y_test):\n    \"\"\"Save all CountVectorizer processed data and vectorizer to disk\"\"\"\n\n    print(\"Saving CountVectorizer processed data to disk...\")\n\n    # Save vectorizer\n    vectorizer_path = os.path.join(ARTIFACT_DIR, \"count_vectorizer.joblib\")\n    joblib.dump(count_vectorizer, vectorizer_path)\n    print(f\"✓ Saved Count vectorizer to {vectorizer_path}\")\n\n    # Save vectorized training data\n    train_count_path = os.path.join(ARTIFACT_DIR, \"X_train_count.npz\")\n    save_npz(train_count_path, X_train_count)\n    print(f\"✓ Saved training Count data to {train_count_path}\")\n\n    # Save vectorized test data\n    test_count_path = os.path.join(ARTIFACT_DIR, \"X_test_count.npz\")\n    save_npz(test_count_path, X_test_count)\n    print(f\"✓ Saved test Count data to {test_count_path}\")\n\n    # Save target variables\n    y_train_np = y_train.to_numpy() if hasattr(\n        y_train, \"to_numpy\") else np.asarray(y_train)\n    y_test_np = y_test.to_numpy() if hasattr(\n        y_test, \"to_numpy\") else np.asarray(y_test)\n\n    y_train_count_path = os.path.join(ARTIFACT_DIR, \"y_train_count.npy\")\n    y_test_count_path = os.path.join(ARTIFACT_DIR, \"y_test_count.npy\")\n\n    np.save(y_train_count_path, y_train_np)\n    np.save(y_test_count_path, y_test_np)\n    print(f\"✓ Saved training targets to {y_train_count_path}\")\n    print(f\"✓ Saved test targets to {y_test_count_path}\")\n\n    # Print file sizes for verification\n    print(\"\\nCountVectorizer file sizes:\")\n    for filename in [\"count_vectorizer.joblib\", \"X_train_count.npz\", \"X_test_count.npz\", \"y_train_count.npy\", \"y_test_count.npy\"]:\n        filepath = os.path.join(ARTIFACT_DIR, filename)\n        if os.path.exists(filepath):\n            size_mb = os.path.getsize(filepath) / (1024 * 1024)\n            print(f\"  {filename}: {size_mb:.2f} MB\")\n\n    print(\n        f\"\\n✅ All CountVectorizer data saved successfully to '{ARTIFACT_DIR}'\")\n\n    return {\n        'vectorizer_path': vectorizer_path,\n        'train_count_path': train_count_path,\n        'test_count_path': test_count_path,\n        'y_train_path': y_train_count_path,\n        'y_test_path': y_test_count_path\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T13:01:48.622113Z","iopub.execute_input":"2025-08-10T13:01:48.622363Z","iopub.status.idle":"2025-08-10T13:01:48.634782Z","shell.execute_reply.started":"2025-08-10T13:01:48.622343Z","shell.execute_reply":"2025-08-10T13:01:48.633738Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save all processed CountVectorizer data\nsaved_paths = save_count_processed_data(\n    count_vectorizer, X_train_count, X_test_count, y_train, y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T13:01:48.635679Z","iopub.execute_input":"2025-08-10T13:01:48.635937Z","iopub.status.idle":"2025-08-10T13:03:40.316267Z","shell.execute_reply.started":"2025-08-10T13:01:48.635912Z","shell.execute_reply":"2025-08-10T13:03:40.315163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_count_processed_data():\n    \"\"\"Load previously saved CountVectorizer processed data for model training\"\"\"\n\n    print(\"Loading CountVectorizer processed data from disk...\")\n\n    # Check if all required files exist\n    required_files = [\n        \"count_vectorizer.joblib\",\n        \"X_train_count.npz\",\n        \"X_test_count.npz\",\n        \"y_train_count.npy\",\n        \"y_test_count.npy\"\n    ]\n\n    missing_files = []\n    for filename in required_files:\n        filepath = os.path.join(ARTIFACT_DIR, filename)\n        if not os.path.exists(filepath):\n            missing_files.append(filename)\n\n    if missing_files:\n        print(f\"❌ Missing files: {missing_files}\")\n        print(\"Please run the CountVectorizer data processing cells first.\")\n        return None\n\n    # Load vectorizer\n    vectorizer_path = os.path.join(ARTIFACT_DIR, \"count_vectorizer.joblib\")\n    loaded_vectorizer = joblib.load(vectorizer_path)\n    print(f\"✓ Loaded Count vectorizer from {vectorizer_path}\")\n\n    # Load vectorized data\n    train_count_path = os.path.join(ARTIFACT_DIR, \"X_train_count.npz\")\n    test_count_path = os.path.join(ARTIFACT_DIR, \"X_test_count.npz\")\n\n    loaded_X_train_count = load_npz(train_count_path)\n    loaded_X_test_count = load_npz(test_count_path)\n    print(f\"✓ Loaded training Count data: {loaded_X_train_count.shape}\")\n    print(f\"✓ Loaded test Count data: {loaded_X_test_count.shape}\")\n\n    # Load target variables\n    y_train_path = os.path.join(ARTIFACT_DIR, \"y_train_count.npy\")\n    y_test_path = os.path.join(ARTIFACT_DIR, \"y_test_count.npy\")\n\n    loaded_y_train = np.load(y_train_path)\n    loaded_y_test = np.load(y_test_path)\n    print(f\"✓ Loaded training targets: {loaded_y_train.shape}\")\n    print(f\"✓ Loaded test targets: {loaded_y_test.shape}\")\n\n    print(f\"\\n✅ All CountVectorizer data loaded successfully!\")\n\n    return {\n        'vectorizer': loaded_vectorizer,\n        'X_train_count': loaded_X_train_count,\n        'X_test_count': loaded_X_test_count,\n        'y_train': loaded_y_train,\n        'y_test': loaded_y_test\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T13:03:40.317353Z","iopub.execute_input":"2025-08-10T13:03:40.317689Z","iopub.status.idle":"2025-08-10T13:03:40.327982Z","shell.execute_reply.started":"2025-08-10T13:03:40.317647Z","shell.execute_reply":"2025-08-10T13:03:40.326894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_models_on_count_data(X_train_count, X_test_count, y_train, y_test):\n    \"\"\"Train models using the CountVectorizer processed data\"\"\"\n\n    count_results = {}\n\n    # Define classifiers for Count data\n    classifiers_count = {\n        'Random Forest (Count)': RandomForestClassifier(n_estimators=100, random_state=42),\n        'XGBoost (Count)': xgb.XGBClassifier(random_state=42, verbosity=0),\n        'LightGBM (Count)': lgb.LGBMClassifier(random_state=42, verbose=-1),\n        'Extra Trees (Count)': ExtraTreesClassifier(n_estimators=100, random_state=42),\n        'Naive Bayes (Count)': MultinomialNB(),\n        'Logistic Regression (Count)': LogisticRegression(random_state=42, max_iter=1000)\n    }\n\n    print(\"Training and evaluating models on CountVectorizer data...\")\n    print(\"=\" * 60)\n    print(f\"Training data shape: {X_train_count.shape}\")\n    print(f\"Test data shape: {X_test_count.shape}\")\n    print(f\"Models will be saved to: {ARTIFACT_DIR}\")\n    print(\"=\" * 60)\n\n    # Train Count models\n    for name, clf in classifiers_count.items():\n        print(f\"\\n🚀 Training {name}...\")\n        try:\n            # Train the model\n            clf.fit(X_train_count, y_train)\n\n            # Make predictions\n            y_pred = clf.predict(X_test_count)\n\n            # Calculate metrics\n            accuracy = accuracy_score(y_test, y_pred)\n            f1 = f1_score(y_test, y_pred)\n\n            # Store results\n            count_results[name] = {\n                'accuracy': accuracy,\n                'f1_score': f1,\n                'predictions': y_pred,\n                'model': clf\n            }\n\n            print(f\"   ✅ Accuracy: {accuracy:.4f}\")\n            print(f\"   ✅ F1 Score: {f1:.4f}\")\n\n            # Save the trained model\n            model_filename = f\"{name.replace(' ', '_').replace('(', '').replace(')', '').lower()}.joblib\"\n            model_path = os.path.join(ARTIFACT_DIR, model_filename)\n            joblib.dump(clf, model_path)\n            print(f\"   💾 Saved model to {model_filename}\")\n\n        except Exception as e:\n            print(f\"   ❌ Error training {name}: {e}\")\n\n    print(f\"\\n{'='*60}\")\n    print(\"🎉 CountVectorizer model training completed!\")\n\n    return count_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T13:03:40.329005Z","iopub.execute_input":"2025-08-10T13:03:40.329323Z","iopub.status.idle":"2025-08-10T13:03:40.354802Z","shell.execute_reply.started":"2025-08-10T13:03:40.329300Z","shell.execute_reply":"2025-08-10T13:03:40.353842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_count_vectorizer_results(count_results):\n    \"\"\"Analyze and summarize CountVectorizer model performance\"\"\"\n\n    print(\"🔍 COUNTVECTORIZER MODEL ANALYSIS\")\n    print(\"=\" * 50)\n\n    # Create analysis dataframe\n    analysis_data = []\n    for name, result in count_results.items():\n        model_name = name.replace(\" (Count)\", \"\")\n\n        analysis_data.append({\n            'Model': model_name,\n            'Accuracy': result['accuracy'],\n            'F1 Score': result['f1_score'],\n            'Full Name': name\n        })\n\n    analysis_df = pd.DataFrame(analysis_data)\n    analysis_df = analysis_df.sort_values('Accuracy', ascending=False)\n\n    print(\"📊 DETAILED RESULTS:\")\n    print(analysis_df[['Model', 'Accuracy', 'F1 Score']\n                      ].to_string(index=False))\n\n    # Find best model\n    best_model_row = analysis_df.iloc[0]\n    best_model_name = best_model_row['Full Name']\n    best_model = count_results[best_model_name]['model']\n\n    print(f\"\\n🏆 BEST MODEL:\")\n    print(f\"   Model: {best_model_row['Model']}\")\n    print(f\"   Accuracy: {best_model_row['Accuracy']:.4f}\")\n    print(f\"   F1 Score: {best_model_row['F1 Score']:.4f}\")\n\n    # Performance statistics\n    print(f\"\\n📈 PERFORMANCE STATISTICS:\")\n    print(f\"   Average Accuracy: {analysis_df['Accuracy'].mean():.4f}\")\n    print(f\"   Best Accuracy: {analysis_df['Accuracy'].max():.4f}\")\n    print(f\"   Worst Accuracy: {analysis_df['Accuracy'].min():.4f}\")\n    print(f\"   Accuracy Std: {analysis_df['Accuracy'].std():.4f}\")\n    print(f\"   Average F1 Score: {analysis_df['F1 Score'].mean():.4f}\")\n    print(f\"   Best F1 Score: {analysis_df['F1 Score'].max():.4f}\")\n\n    # Visualize performance\n    plt.figure(figsize=(12, 8))\n\n    # Plot 1: Accuracy ranking\n    plt.subplot(2, 2, 1)\n    colors = plt.cm.Reds(np.linspace(0.4, 0.8, len(analysis_df)))\n    plt.barh(range(len(analysis_df)), analysis_df['Accuracy'], color=colors)\n    plt.xlabel('Accuracy')\n    plt.ylabel('Models')\n    plt.title('Model Ranking by Accuracy')\n    plt.yticks(range(len(analysis_df)), analysis_df['Model'])\n    plt.gca().invert_yaxis()\n\n    # Plot 2: F1 Score ranking\n    plt.subplot(2, 2, 2)\n    colors = plt.cm.Greens(np.linspace(0.4, 0.8, len(analysis_df)))\n    plt.barh(range(len(analysis_df)), analysis_df['F1 Score'], color=colors)\n    plt.xlabel('F1 Score')\n    plt.ylabel('Models')\n    plt.title('Model Ranking by F1 Score')\n    plt.yticks(range(len(analysis_df)), analysis_df['Model'])\n    plt.gca().invert_yaxis()\n\n    # Plot 3: Accuracy vs F1 scatter\n    plt.subplot(2, 2, 3)\n    plt.scatter(analysis_df['Accuracy'], analysis_df['F1 Score'],\n                c=range(len(analysis_df)), cmap='viridis', s=100)\n    plt.xlabel('Accuracy')\n    plt.ylabel('F1 Score')\n    plt.title('Accuracy vs F1 Score')\n    for i, model in enumerate(analysis_df['Model']):\n        plt.annotate(model, (analysis_df.iloc[i]['Accuracy'], analysis_df.iloc[i]['F1 Score']),\n                     xytext=(5, 5), textcoords='offset points', fontsize=8)\n\n    # Plot 4: Performance distribution\n    plt.subplot(2, 2, 4)\n    plt.hist([analysis_df['Accuracy'], analysis_df['F1 Score']],\n             bins=10, alpha=0.7, label=['Accuracy', 'F1 Score'])\n    plt.xlabel('Score')\n    plt.ylabel('Frequency')\n    plt.title('Performance Distribution')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    return analysis_df, best_model_name, best_model\n\n# Note: Use this function after training CountVectorizer models\n# analysis_df, best_model_name, best_model = analyze_count_vectorizer_results(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T13:03:40.356006Z","iopub.execute_input":"2025-08-10T13:03:40.356367Z","iopub.status.idle":"2025-08-10T13:03:40.380829Z","shell.execute_reply.started":"2025-08-10T13:03:40.356338Z","shell.execute_reply":"2025-08-10T13:03:40.379722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🚀 COMPLETE COUNTVECTORIZER WORKFLOW\n\ndef run_complete_count_vectorization_workflow():\n    \"\"\"\n    Complete workflow to process data with CountVectorizer and train models\n    Run this after the initial data loading and train-test split\n    \"\"\"\n\n    print(\"🎯 STARTING COUNTVECTORIZER WORKFLOW\")\n    print(\"=\" * 50)\n\n    # Check if we have the basic data\n    if 'X_train' not in globals() or 'X_test' not in globals():\n        print(\"❌ Train-test split data not found. Please run the data loading cells first.\")\n        return False\n\n    # Step 1: Process with CountVectorizer\n    print(\"\\n📊 STEP 1: CountVectorizer Processing\")\n    if 'count_vectorizer' not in globals():\n        print(\"Processing with CountVectorizer...\")\n        global count_vectorizer, X_train_count, X_test_count\n        count_vectorizer, X_train_count, X_test_count = vectorize_data_count_in_batches(\n            X_train, X_test, y_train, y_test)\n        save_count_processed_data(\n            count_vectorizer, X_train_count, X_test_count, y_train, y_test)\n    else:\n        print(\"✅ CountVectorizer data already available\")\n\n    # Step 2: Train models on CountVectorizer data\n    print(\"\\n🤖 STEP 2: Training CountVectorizer Models\")\n    if 'results' not in globals():\n        global results\n        results = train_models_on_count_data(\n            X_train_count, X_test_count, y_train, y_test)\n    else:\n        print(\"✅ CountVectorizer models already trained\")\n\n    # Step 3: Analyze results\n    print(\"\\n📈 STEP 3: Analyzing Results\")\n    analysis_df, best_model_name, best_model = analyze_count_vectorizer_results(\n        results)\n\n    # Step 4: Test predictions\n    print(\"\\n🧪 STEP 4: Testing Predictions\")\n    test_prediction_functions()\n\n    print(\"\\n🎉 COUNTVECTORIZER WORKFLOW COMPLETED!\")\n    print(\"=\" * 50)\n\n    return True\n\n\n# Instructions for usage:\nprint(\"\"\"\n📋 COUNTVECTORIZER WORKFLOW INSTRUCTIONS:\n\n1. FIRST RUN (Complete Processing):\n   - Run cells 1-6 to load data and create train-test split\n   - Run: run_complete_count_vectorization_workflow()\n   - This will process data with CountVectorizer and train all models\n\n2. SUBSEQUENT RUNS (Load Saved Data):\n   - Uncomment and run the data loading section\n   - Load Count data: load_count_processed_data()\n   - Analyze results: analyze_count_vectorizer_results(results)\n\n3. INDIVIDUAL STEPS:\n   - Data processing: vectorize_data_count_in_batches()\n   - Model training: train_models_on_count_data()\n   - Results analysis: analyze_count_vectorizer_results()\n   - Prediction testing: test_prediction_functions()\n\n💡 MEMORY MANAGEMENT TIPS:\n- CountVectorizer processes data in 2000-row batches\n- Intermediate files are cleaned up automatically\n- Use gc.collect() between steps if memory is limited\n- All processed data is saved to {ARTIFACT_DIR} for future use\n\n🎯 COUNTVECTORIZER ADVANTAGES:\n- Simple and fast text vectorization\n- Preserves exact word frequencies\n- Less memory intensive during processing\n- Good baseline approach for text classification\n- Efficient for large datasets with batch processing\n\"\"\")\n\n# Uncomment the line below to run the complete workflow:\n# run_complete_count_vectorization_workflow()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T13:03:40.385020Z","iopub.execute_input":"2025-08-10T13:03:40.385423Z","iopub.status.idle":"2025-08-10T13:03:40.405170Z","shell.execute_reply.started":"2025-08-10T13:03:40.385386Z","shell.execute_reply":"2025-08-10T13:03:40.404117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 📁 COUNTVECTORIZER DATA ORGANIZATION\n\ndef show_count_directory_structure():\n    \"\"\"Display the CountVectorizer data directory structure\"\"\"\n    \n    print(\"📁 COUNTVECTORIZER DATA ORGANIZATION\")\n    print(\"=\" * 40)\n    \n    # Check CountVectorizer directory\n    count_dir = ARTIFACT_DIR\n    \n    print(f\"🔸 CountVectorizer Directory: {count_dir}\")\n    if os.path.exists(count_dir):\n        count_files = [f for f in os.listdir(count_dir) if os.path.isfile(os.path.join(count_dir, f))]\n        print(f\"   ✓ Contains {len(count_files)} files\")\n        if count_files:\n            total_size = 0\n            for file in sorted(count_files):\n                size_mb = os.path.getsize(os.path.join(count_dir, file)) / (1024 * 1024)\n                total_size += size_mb\n                print(f\"     • {file} ({size_mb:.2f} MB)\")\n            print(f\"   📊 Total size: {total_size:.2f} MB\")\n    else:\n        print(\"   ❌ Directory does not exist\")\n    \n    print(f\"\\n📊 EXPECTED DIRECTORY STRUCTURE:\")\n    print(f\"\"\"\n{ARTIFACT_DIR}\n├── count_vectorizer.joblib         # CountVectorizer\n├── X_train_count.npz               # Training features\n├── X_test_count.npz                # Test features\n├── y_train_count.npy               # Training labels\n├── y_test_count.npy                # Test labels\n├── random_forest_count.joblib      # Trained Random Forest model\n├── xgboost_count.joblib            # Trained XGBoost model\n├── lightgbm_count.joblib           # Trained LightGBM model\n├── extra_trees_count.joblib        # Trained Extra Trees model\n├── naive_bayes_count.joblib        # Trained Naive Bayes model\n└── logistic_regression_count.joblib # Trained Logistic Regression model\n\"\"\")\n\ndef verify_count_data_integrity():\n    \"\"\"Verify the integrity of CountVectorizer processed data\"\"\"\n    \n    print(\"\udd0d VERIFYING COUNTVECTORIZER DATA INTEGRITY\")\n    print(\"=\" * 45)\n    \n    required_files = [\n        \"count_vectorizer.joblib\",\n        \"X_train_count.npz\", \n        \"X_test_count.npz\",\n        \"y_train_count.npy\",\n        \"y_test_count.npy\"\n    ]\n    \n    missing_files = []\n    existing_files = []\n    \n    for filename in required_files:\n        filepath = os.path.join(ARTIFACT_DIR, filename)\n        if os.path.exists(filepath):\n            existing_files.append(filename)\n            # Check file size\n            size_mb = os.path.getsize(filepath) / (1024 * 1024)\n            print(f\"✓ {filename} ({size_mb:.2f} MB)\")\n        else:\n            missing_files.append(filename)\n            print(f\"❌ {filename} - MISSING\")\n    \n    print(f\"\\n📊 SUMMARY:\")\n    print(f\"   Found: {len(existing_files)}/{len(required_files)} required files\")\n    \n    if missing_files:\n        print(f\"   ❌ Missing files: {missing_files}\")\n        print(\"   Please run the CountVectorizer processing pipeline first.\")\n        return False\n    else:\n        print(\"   ✅ All required files present\")\n        \n        # Check for trained models\n        model_patterns = [\n            \"random_forest_count.joblib\",\n            \"xgboost_count.joblib\", \n            \"lightgbm_count.joblib\",\n            \"extra_trees_count.joblib\",\n            \"naive_bayes_count.joblib\",\n            \"logistic_regression_count.joblib\"\n        ]\n        \n        model_count = 0\n        for pattern in model_patterns:\n            if os.path.exists(os.path.join(ARTIFACT_DIR, pattern)):\n                model_count += 1\n        \n        print(f\"   🤖 Trained models: {model_count}/{len(model_patterns)}\")\n        \n        return True\n\ndef cleanup_count_directory():\n    \"\"\"Clean up temporary or unnecessary files in the CountVectorizer directory\"\"\"\n    \n    print(\"🧹 CLEANING UP COUNTVECTORIZER DIRECTORY\")\n    print(\"=\" * 40)\n    \n    if not os.path.exists(ARTIFACT_DIR):\n        print(\"❌ CountVectorizer directory does not exist\")\n        return\n    \n    # Patterns for temporary files to clean up\n    temp_patterns = [\n        \"*_batch_*.npz\",  # Temporary batch files\n        \"*.tmp\",          # Temporary files\n        \"*.log\"           # Log files\n    ]\n    \n    import glob\n    cleaned_count = 0\n    \n    for pattern in temp_patterns:\n        temp_files = glob.glob(os.path.join(ARTIFACT_DIR, pattern))\n        for temp_file in temp_files:\n            try:\n                os.remove(temp_file)\n                print(f\"🗑️ Removed: {os.path.basename(temp_file)}\")\n                cleaned_count += 1\n            except Exception as e:\n                print(f\"❌ Failed to remove {os.path.basename(temp_file)}: {e}\")\n    \n    if cleaned_count == 0:\n        print(\"✅ No temporary files to clean up\")\n    else:\n        print(f\"✅ Cleaned up {cleaned_count} temporary files\")\n\n# Display current structure\nshow_count_directory_structure()\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"💡 COUNTVECTORIZER ORGANIZATION TIPS:\")\nprint(\"1. Run verify_count_data_integrity() to check data completeness\")\nprint(\"2. Run cleanup_count_directory() to remove temporary files\") \nprint(\"3. All CountVectorizer data is stored in a single directory\")\nprint(\"4. Models are saved with descriptive names for easy identification\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train all models using CountVectorizer data\nresults = train_models_on_count_data(\n    X_train_count, X_test_count, y_train, y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compare CountVectorizer model performances\nprint(\"CountVectorizer Model Performance Comparison:\")\nprint(\"=\" * 60)\n\n# Create comparison dataframe\ncomparison_data = []\nfor name, result in results.items():\n    comparison_data.append({\n        # Remove (Count) suffix for cleaner display\n        'Model': name.replace(' (Count)', ''),\n        'Accuracy': result['accuracy'],\n        'F1 Score': result['f1_score']\n    })\n\ncomparison_df = pd.DataFrame(comparison_data)\ncomparison_df = comparison_df.sort_values('Accuracy', ascending=False)\nprint(comparison_df.to_string(index=False))\n\n# Find the best model\n# Get original name with (Count)\nbest_model_name = list(results.keys())[comparison_df.index[0]]\nbest_model = results[best_model_name]['model']\nprint(f\"\\nBest performing model: {comparison_df.iloc[0]['Model']}\")\nprint(f\"Best accuracy: {comparison_df.iloc[0]['Accuracy']:.4f}\")\nprint(f\"Best F1 score: {comparison_df.iloc[0]['F1 Score']:.4f}\")\n\n# Visualize results\nplt.figure(figsize=(12, 5))\n\n# Plot 1: Accuracy comparison\nplt.subplot(1, 2, 1)\nplt.bar(range(len(comparison_df)),\n        comparison_df['Accuracy'], color='lightcoral')\nplt.xlabel('Models')\nplt.ylabel('Accuracy')\nplt.title('CountVectorizer Model Accuracy Comparison')\nplt.xticks(range(len(comparison_df)),\n           comparison_df['Model'], rotation=45, ha='right')\nplt.ylim(0, 1)\n\n# Plot 2: F1 Score comparison\nplt.subplot(1, 2, 2)\nplt.bar(range(len(comparison_df)),\n        comparison_df['F1 Score'], color='lightgreen')\nplt.xlabel('Models')\nplt.ylabel('F1 Score')\nplt.title('CountVectorizer Model F1 Score Comparison')\nplt.xticks(range(len(comparison_df)),\n           comparison_df['Model'], rotation=45, ha='right')\nplt.ylim(0, 1)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create confusion matrix for the best CountVectorizer model\nbest_predictions = results[best_model_name]['predictions']\n\nplt.figure(figsize=(8, 6))\ncm = confusion_matrix(y_test, best_predictions)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Reds',\n            xticklabels=['Legitimate', 'Phishing'],\n            yticklabels=['Legitimate', 'Phishing'])\nplt.title(\n    f'Confusion Matrix - {comparison_df.iloc[0][\"Model\"]} (CountVectorizer)')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n# Print detailed classification report\nprint(\n    f\"\\nDetailed Classification Report for {comparison_df.iloc[0]['Model']}:\")\nprint(\"=\" * 60)\nprint(classification_report(y_test, best_predictions,\n      target_names=['Legitimate', 'Phishing']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CountVectorizer prediction functions\ndef predict_webpage_status(webpage_code, model=None, vectorizer=None):\n    \"\"\"\n    Predict if a webpage is phishing (1) or legitimate (0) based on its HTML code using CountVectorizer.\n\n    Parameters:\n    webpage_code (str): The HTML code of the webpage\n    model: The trained model to use for prediction (default: best model)\n    vectorizer: The CountVectorizer to use (default: loaded vectorizer)\n\n    Returns:\n    dict: Prediction result with probability scores\n    \"\"\"\n    if model is None:\n        if 'best_model' in globals():\n            model = best_model\n        else:\n            print(\"❌ No model available. Please train models first.\")\n            return None\n\n    if vectorizer is None:\n        if 'count_vectorizer' in globals():\n            vectorizer = count_vectorizer\n        else:\n            print(\"❌ No CountVectorizer available. Please load processed data first.\")\n            return None\n\n    try:\n        # Vectorize the input\n        webpage_vectorized = vectorizer.transform([webpage_code])\n\n        # Make prediction\n        prediction = model.predict(webpage_vectorized)[0]\n\n        # Get prediction probabilities\n        probabilities = model.predict_proba(webpage_vectorized)[0]\n\n        # Create result dictionary\n        result = {\n            'prediction': prediction,\n            'status': 'Phishing' if prediction == 1 else 'Legitimate',\n            'confidence': max(probabilities),\n            'probability_legitimate': probabilities[0],\n            'probability_phishing': probabilities[1],\n            'vectorizer_used': 'CountVectorizer'\n        }\n\n        return result\n\n    except Exception as e:\n        print(f\"❌ Error making prediction: {e}\")\n        return None\n\n\ndef load_model_for_prediction(model_name):\n    \"\"\"Load a specific trained CountVectorizer model for prediction\"\"\"\n    model_filename = f\"{model_name.replace(' ', '_').replace('(', '').replace(')', '').lower()}.joblib\"\n    model_path = os.path.join(ARTIFACT_DIR, model_filename)\n\n    if os.path.exists(model_path):\n        loaded_model = joblib.load(model_path)\n        print(f\"✅ Loaded model: {model_name}\")\n        return loaded_model\n    else:\n        print(f\"❌ Model file not found: {model_path}\")\n        return None\n\n\ndef test_prediction_functions():\n    \"\"\"Test the prediction function with sample data using CountVectorizer\"\"\"\n\n    if 'results' not in globals() or not results:\n        print(\"❌ No trained models available. Please train models first.\")\n        return\n\n    print(f\"Using best model: {comparison_df.iloc[0]['Model']}\")\n    print(f\"Best accuracy: {comparison_df.iloc[0]['Accuracy']:.4f}\")\n\n    # Test with sample HTML codes\n    test_samples = [\n        \"<html><head><title>Google</title></head><body>Welcome to Google</body></html>\",\n        \"<html><head><title>Secure Bank Login</title></head><body><form>Enter password</form></body></html>\",\n        \"<html><script>window.location='phishing-site.com'</script></html>\",\n        \"<html><body>Click here to verify your account: <a href='fake-bank.com'>Verify</a></body></html>\",\n        \"<html><body>Urgent! Your account will be suspended. Click <a href='malicious-bank-site.com'>here</a></body></html>\"\n    ]\n\n    print(\"\\n🧪 TESTING COUNTVECTORIZER PREDICTION FUNCTION\")\n    print(\"=\" * 70)\n\n    for i, sample in enumerate(test_samples, 1):\n        result = predict_webpage_status(sample, best_model, count_vectorizer)\n        if result:\n            print(f\"\\n📄 Sample {i}: {sample[:60]}...\")\n            print(f\"   Prediction: {result['status']}\")\n            print(f\"   Confidence: {result['confidence']:.3f}\")\n            print(\n                f\"   Prob Legitimate: {result['probability_legitimate']:.3f}\")\n            print(f\"   Prob Phishing: {result['probability_phishing']:.3f}\")\n        else:\n            print(f\"\\n📄 Sample {i}: ❌ Prediction failed\")\n\n# Note: Run test_prediction_functions() after training models","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🎯 CountVectorizer Batch Processing Workflow Summary\nprint(\"\"\"\n📊 COUNTVECTORIZER BATCH PROCESSING WORKFLOW FOR PHISHING DETECTION\n================================================================\n\nThis notebook implements an efficient batch processing approach for handling large datasets using CountVectorizer:\n\n🔄 WORKFLOW STEPS:\n1. ✅ Load dataset in chunks (2000 rows at a time)\n2. ✅ Analyze dataset structure and statistics \n3. ✅ Create stratified train-test split\n4. ✅ Vectorize data in batches using CountVectorizer\n5. ✅ Save vectorized data and vectorizer to disk\n6. ✅ Train multiple models on processed data\n7. ✅ Save trained models for future use\n8. ✅ Evaluate and compare model performance\n\n💾 SAVED ARTIFACTS (in /kaggle/working/count/):\n- count_vectorizer.joblib          (CountVectorizer)\n- X_train_count.npz               (Training features)\n- X_test_count.npz                (Test features) \n- y_train_count.npy               (Training labels)\n- y_test_count.npy                (Test labels)\n- [model_name].joblib             (Trained models)\n\n🚀 MEMORY MANAGEMENT:\n- Processes data in 2000-row batches\n- Saves intermediate results to disk\n- Cleans up memory after each batch\n- Monitors memory usage throughout\n\n📈 MODELS TRAINED:\n- Random Forest\n- XGBoost  \n- LightGBM\n- Extra Trees\n- Naive Bayes\n- Logistic Regression\n\n🎯 USAGE:\n1. Run all cells in sequence to process data and train models\n2. Use load_count_processed_data() to reload saved data\n3. Use predict_webpage_status() for new predictions\n4. Use test_prediction_functions() to test the prediction pipeline\n\n💡 COUNTVECTORIZER BENEFITS:\n- Fast and simple text vectorization\n- Preserves exact word counts\n- Less computationally intensive than TF-IDF\n- Good baseline for text classification\n- Handles large datasets efficiently in batches\n\"\"\")\n\n# Print current status\nprint(f\"\\n📍 CURRENT STATUS:\")\nprint(f\"Working directory: {os.getcwd()}\")\nprint(f\"Artifact directory: {ARTIFACT_DIR}\")\nprint(f\"Dataset path: {dataset_path}\")\n\n# Check if processed data exists\nif os.path.exists(os.path.join(ARTIFACT_DIR, \"count_vectorizer.joblib\")):\n    print(\"✅ CountVectorizer processed data available\")\nelse:\n    print(\"⏳ CountVectorizer processed data not yet created - run processing cells first\")\n\n# Memory status\nprint_memory_usage(\"(current)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Phishing URL Detection","metadata":{}},{"cell_type":"code","source":"# Check if we're running locally or on Kaggle for URL dataset\nif os.path.exists(\"/home/iqbal/Programming/ML/project/dataset/new_data_urls.csv\"):\n    url_dataset_path = \"/home/iqbal/Programming/ML/project/dataset/new_data_urls.csv\"\nelse:\n    url_dataset_path = \"/kaggle/input/phising-website-url-dataset/new_data_urls.csv\"\n\nurl_dataSet = pd.read_csv(url_dataset_path)\nurl_dataSet.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare features and target for URL dataset\nfrom sklearn.model_selection import train_test_split\n\nurl_X = url_dataSet['url']\nurl_y = url_dataSet['status']\n\nprint(f\"URL dataset shape: {url_dataSet.shape}\")\nprint(f\"Class distribution:\\n{url_y.value_counts()}\")\n\n# Split into train/test sets\nurl_X_train, url_X_test, url_y_train, url_y_test = train_test_split(\n    url_X, url_y, test_size=0.2, random_state=42, stratify=url_y\n)\nprint(f\"Train size: {len(url_X_train)}, Test size: {len(url_X_test)}\")\n\n# Create pipelines for URL classification\npipelines = {\n    'CountVectorizer + Random Forest': Pipeline([\n        ('vectorizer', CountVectorizer(max_features=5000, stop_words='english')),\n        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n    ]),\n    'CountVectorizer + XGBoost': Pipeline([\n        ('vectorizer', CountVectorizer(max_features=5000, stop_words='english')),\n        ('classifier', xgb.XGBClassifier(random_state=42, verbosity=0))\n    ]),\n    'CountVectorizer + Naive Bayes': Pipeline([\n        ('vectorizer', CountVectorizer(max_features=5000, stop_words='english')),\n        ('classifier', MultinomialNB())\n    ]),\n    'CountVectorizer + Logistic Regression': Pipeline([\n        ('vectorizer', CountVectorizer(max_features=5000, stop_words='english')),\n        ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n    ])\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n\n\nurl_results = {}\nprint(\"Training and evaluating URL models...\")\nfor name, pipeline in pipelines.items():\n    print(f\"\\nTraining {name}...\")\n    try:\n        pipeline.fit(url_X_train, url_y_train)\n        y_pred = pipeline.predict(url_X_test)\n        acc = accuracy_score(url_y_test, y_pred)\n        f1 = f1_score(url_y_test, y_pred)\n        url_results[name] = {'accuracy': acc, 'f1_score': f1,\n                             'predictions': y_pred, 'model': pipeline}\n        print(f\"Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n    except Exception as e:\n        print(f\"Error training {name}: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compare URL model performances\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncomparison_url = []\nfor name, result in url_results.items():\n    comparison_url.append({\n        'Model': name,\n        'Accuracy': result['accuracy'],\n        'F1 Score': result['f1_score']\n    })\n\ncomparison_url_df = pd.DataFrame(\n    comparison_url).sort_values('Accuracy', ascending=False)\nprint(comparison_url_df.to_string(index=False))\n\nbest_url_model_name = comparison_url_df.iloc[0]['Model']\nbest_url_model = url_results[best_url_model_name]['model']\nprint(f\"\\nBest URL model: {best_url_model_name}\")\nprint(f\"Accuracy: {comparison_url_df.iloc[0]['Accuracy']:.4f}\")\n\n# Visualize accuracy and F1 score\nplt.figure(figsize=(10, 4))\nplt.bar(comparison_url_df['Model'], comparison_url_df['Accuracy'],\n        color='skyblue', label='Accuracy')\nplt.bar(comparison_url_df['Model'], comparison_url_df['F1 Score'],\n        color='lightcoral', alpha=0.7, label='F1 Score')\nplt.xticks(rotation=45, ha='right')\nplt.ylabel('Score')\nplt.title('Phishing URL Model Performance')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Confusion matrix for best model\ny_pred_best = url_results[best_url_model_name]['predictions']\ncm = confusion_matrix(url_y_test, y_pred_best)\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\n            'Legitimate', 'Phishing'], yticklabels=['Legitimate', 'Phishing'])\nplt.title(f'Confusion Matrix - {best_url_model_name}')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a function to predict URL status\ndef predict_url_status(url, model=None):\n    \"\"\"\n    Predict if a URL is phishing (1) or legitimate (0) based on its text.\n\n    Parameters:\n    url (str): The URL to predict\n    model: The trained model to use for prediction (default: best model)\n\n    Returns:\n    dict: Prediction result with probability scores\n    \"\"\"\n    if model is None:\n        model = best_url_model\n\n    # Make prediction\n    prediction = model.predict([url])[0]\n\n    # Get prediction probabilities\n    probabilities = model.predict_proba([url])[0]\n\n    # Create result dictionary\n    result = {\n        'prediction': prediction,\n        'status': 'Legitimate' if prediction == 1 else 'Phishing',\n        'confidence': max(probabilities),\n        'probability_legitimate': probabilities[0],\n        'probability_phishing': probabilities[1]\n    }\n\n    return result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"url_sample = [\"google.com\", \"facebook.com\", \"phishing-test.com\",\n              \"example.com\", \"malicious-site.com\", 'facebook-test.com']\n\nprint(\"\\nTesting URL prediction function:\")\nfor url in url_sample:\n    result = predict_url_status(url)\n    print(f\"URL: {url} | Prediction: {result['status']} | \"\n          f\"Confidence: {result['confidence']:.4f} | \"\n          f\"Prob Legitimate: {result['probability_legitimate']:.4f} | \"\n          f\"Prob Phishing: {result['probability_phishing']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}